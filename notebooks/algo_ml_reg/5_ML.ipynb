{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c15721e-cfbe-4ada-af7f-35f9d6ee150a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Previous Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513a9355-3926-40f4-9fb5-afc9c2777a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## True if you want to import data downloaded previusly\n",
    "# flag_loading2 = True\n",
    "\n",
    "# History Prices Interval:\n",
    "# 1 = 5m\n",
    "# 2 = 1h\n",
    "# 3 = 1d\n",
    "inter2 = 2\n",
    "\n",
    "# Update history dispite it was downloaded today\n",
    "flag_history_download2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "232c7af7-b0dc-463a-bea1-f4ed899276a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. HISTORY\n",
      "\n",
      "---------------\n",
      "1.1 Header\n",
      "---------------\n",
      "\n",
      "Not loading updated files\n",
      "Loading outdated files\n",
      "Updating History Data, dispite the date and time\n",
      "History interval: 1 HOUR\n",
      "Last operations file loaded\n",
      "Market = portfolio_world_index_crypto_currencies_top1000_mcapT22022\n",
      "\n",
      "---------------\n",
      "1.2 Files Loading\n",
      "---------------\n",
      "\n",
      "HISTORY loaded\n",
      "flag_history: True\n",
      "GRAL DATA file loaded\n",
      "Oudated STATS loaded\n",
      "Oudated STATS_VAL loaded\n",
      "Oudated EARNINGS loaded\n",
      "\n",
      "---------------\n",
      "1.3 Data Download\n",
      "---------------\n",
      "\n",
      "Stats data already gotten or not needed\n",
      "Stats Val data already gotten or not needed\n",
      "General data already gotten\n",
      "EUR-USD downloaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a35b87a7bda47f1801d30294ade9715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Label(value='Processing Prices --> ', layout=Layout(width='12%')), IntProgress(value=0, layout=Lâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ANTM: No data found, symbol may be delisted\n",
      "ANTM's History not working\n",
      "- GAZP.ME: No data found for this date range, symbol may be delisted\n",
      "GAZP.ME's History not working\n",
      "- ROSN.ME: No data found for this date range, symbol may be delisted\n",
      "ROSN.ME's History not working\n",
      "- LKOH.ME: No data found for this date range, symbol may be delisted\n",
      "LKOH.ME's History not working\n",
      "- NVTK.ME: No data found for this date range, symbol may be delisted\n",
      "NVTK.ME's History not working\n",
      "- ACC: No data found for this date range, symbol may be delisted\n",
      "ACC's History not working\n",
      "History Downloaded! - Process time: 387 seconds (6.45 minutes)\n",
      "Earnings data already gotten or no needed\n",
      "\n",
      "---------------\n",
      "1.4 Data Wrangling\n",
      "---------------\n",
      "\n",
      "Data already checked\n",
      "Earnings data already edited or there are no earnings\n",
      "\n",
      "---------------\n",
      "1.6 Export\n",
      "---------------\n",
      "\n",
      "portfolio_world_index_crypto_currencies_top1000_mcapT22022 History exported\n",
      "Nothing new to export\n",
      "Nothing new to export\n",
      "Nothing new to export\n",
      "Nothing new to export\n",
      "\n",
      "2. STRATEGY\n",
      "\n",
      "---------------\n",
      "2.1 Operations DF Update\n",
      "---------------\n",
      "\n",
      "There is no new activity\n",
      "There is no new activity\n",
      "Current prices Updated\n",
      "\n",
      "---------------\n",
      "2.2 Export\n",
      "---------------\n",
      "\n",
      "Nothing new to export\n",
      "\n",
      "3. TECNICAL ANALYSIS\n",
      "\n",
      "---------------\n",
      "3.1 Calculations\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23680\\1883624257.py:8\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m returns_cartera \u001b[38;5;241m=\u001b[39m returns\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m volatility_cartera_tick \u001b[38;5;241m=\u001b[39m (returns_cartera\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m volatility_cartera \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvolatility_cartera_tick\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshares_norm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\numpy\\lib\\function_base.py:409\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned)\u001b[0m\n\u001b[0;32m    407\u001b[0m     scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m--> 409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    412\u001b[0m     avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(a, wgt, dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\u001b[38;5;241m.\u001b[39msum(axis)\u001b[38;5;241m/\u001b[39mscl\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23680\\157821589.py:1\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3_TEC_ANALYSIS.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2305\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2303\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2304\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2305\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\magics\\execution.py:717\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2811\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[1;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[0;32m   2809\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[1;32m-> 2811\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m   2813\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:251\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23680\\1883624257.py:8\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m returns_cartera \u001b[38;5;241m=\u001b[39m returns\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m volatility_cartera_tick \u001b[38;5;241m=\u001b[39m (returns_cartera\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m volatility_cartera \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvolatility_cartera_tick\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshares_norm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\numpy\\lib\\function_base.py:409\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned)\u001b[0m\n\u001b[0;32m    407\u001b[0m     scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m--> 409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    412\u001b[0m     avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(a, wgt, dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\u001b[38;5;241m.\u001b[39msum(axis)\u001b[38;5;241m/\u001b[39mscl\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "Weights sum to zero, can't be normalized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m4_TICKERS.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2305\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2303\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2304\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2305\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\magics\\execution.py:717\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2811\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[1;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[0;32m   2809\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[1;32m-> 2811\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m   2813\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:251\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23680\\157821589.py:1\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3_TEC_ANALYSIS.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2305\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2303\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2304\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2305\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\magics\\execution.py:717\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2811\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[1;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[0;32m   2809\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[1;32m-> 2811\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[0;32m   2813\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\IPython\\core\\interactiveshell.py:251\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23680\\1883624257.py:8\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m returns_cartera \u001b[38;5;241m=\u001b[39m returns\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m volatility_cartera_tick \u001b[38;5;241m=\u001b[39m (returns_cartera\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m volatility_cartera \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvolatility_cartera_tick\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshares_norm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\PY\\lib\\site-packages\\numpy\\lib\\function_base.py:409\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned)\u001b[0m\n\u001b[0;32m    407\u001b[0m     scl \u001b[38;5;241m=\u001b[39m wgt\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(scl \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m--> 409\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights sum to zero, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    412\u001b[0m     avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(a, wgt, dtype\u001b[38;5;241m=\u001b[39mresult_dtype)\u001b[38;5;241m.\u001b[39msum(axis)\u001b[38;5;241m/\u001b[39mscl\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m returned:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: Weights sum to zero, can't be normalized"
     ]
    }
   ],
   "source": [
    "%run 4_TICKERS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10cb4ebb-1724-427e-9f15-6627d2e1a9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net_(eur)</th>\n",
       "      <th>shares</th>\n",
       "      <th>returns_(eur)</th>\n",
       "      <th>sales_num</th>\n",
       "      <th>purchases_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>-631.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMW.DE</th>\n",
       "      <td>-630.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-USD</th>\n",
       "      <td>-1675.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ETH-USD</th>\n",
       "      <td>-515.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>-630.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>-630.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVDA</th>\n",
       "      <td>-630.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHEL.L</th>\n",
       "      <td>-100.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLA</th>\n",
       "      <td>-647.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         net_(eur)  shares  returns_(eur)  sales_num  purchases_num\n",
       "ticker                                                             \n",
       "AAPL       -631.70     0.0            0.0          0              2\n",
       "BMW.DE     -630.00     0.0            0.0          0              1\n",
       "BTC-USD   -1675.47     0.0            0.0          0              4\n",
       "ETH-USD    -515.80     0.0            0.0          0              1\n",
       "GOOGL      -630.00     0.0            0.0          0              1\n",
       "MSFT       -630.00     0.0            0.0          0              2\n",
       "NVDA       -630.00     0.0            0.0          0              1\n",
       "SHEL.L     -100.00     0.0            0.0          0              1\n",
       "TSLA       -647.30     0.0            0.0          0              2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa1a8711-be24-49bc-b299-4801a92e1abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>name</th>\n",
       "      <th>exchange</th>\n",
       "      <th>country</th>\n",
       "      <th>currency</th>\n",
       "      <th>net_(eur)</th>\n",
       "      <th>operation_price</th>\n",
       "      <th>operation_price_(eur)</th>\n",
       "      <th>shares</th>\n",
       "      <th>current_price</th>\n",
       "      <th>current_price_(eur)</th>\n",
       "      <th>returns_(eur)</th>\n",
       "      <th>sales_num</th>\n",
       "      <th>purchases_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-10-18</th>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>Bitcoin USD</td>\n",
       "      <td>CCC</td>\n",
       "      <td>ccc</td>\n",
       "      <td>USD</td>\n",
       "      <td>-319.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19790.857244</td>\n",
       "      <td>19808.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-31</th>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>Ethereum USD</td>\n",
       "      <td>CCC</td>\n",
       "      <td>ccc</td>\n",
       "      <td>USD</td>\n",
       "      <td>-515.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1472.764744</td>\n",
       "      <td>1474.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-02</th>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>Bitcoin USD</td>\n",
       "      <td>CCC</td>\n",
       "      <td>ccc</td>\n",
       "      <td>USD</td>\n",
       "      <td>-125.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19790.857244</td>\n",
       "      <td>19808.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-20</th>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>Bitcoin USD</td>\n",
       "      <td>CCC</td>\n",
       "      <td>ccc</td>\n",
       "      <td>USD</td>\n",
       "      <td>-146.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19790.857244</td>\n",
       "      <td>19808.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-03</th>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>Bitcoin USD</td>\n",
       "      <td>CCC</td>\n",
       "      <td>ccc</td>\n",
       "      <td>USD</td>\n",
       "      <td>-1084.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19790.857244</td>\n",
       "      <td>19808.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-09</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "      <td>NasdaqGS</td>\n",
       "      <td>us</td>\n",
       "      <td>USD</td>\n",
       "      <td>-517.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>303.750000</td>\n",
       "      <td>304.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-13</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>NasdaqGS</td>\n",
       "      <td>us</td>\n",
       "      <td>USD</td>\n",
       "      <td>-500.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.370000</td>\n",
       "      <td>152.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-20</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corporation</td>\n",
       "      <td>NasdaqGS</td>\n",
       "      <td>us</td>\n",
       "      <td>USD</td>\n",
       "      <td>-500.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>245.380000</td>\n",
       "      <td>245.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>NasdaqGS</td>\n",
       "      <td>us</td>\n",
       "      <td>USD</td>\n",
       "      <td>-131.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152.370000</td>\n",
       "      <td>152.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>BMW.DE</td>\n",
       "      <td>BAYERISCHE MOTOREN WERKE AG</td>\n",
       "      <td>XETRA</td>\n",
       "      <td>de</td>\n",
       "      <td>EUR</td>\n",
       "      <td>-630.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.680000</td>\n",
       "      <td>74.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>NasdaqGS</td>\n",
       "      <td>us</td>\n",
       "      <td>USD</td>\n",
       "      <td>-630.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.910000</td>\n",
       "      <td>103.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corporation</td>\n",
       "      <td>NasdaqGS</td>\n",
       "      <td>us</td>\n",
       "      <td>USD</td>\n",
       "      <td>-130.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>245.380000</td>\n",
       "      <td>245.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>NVIDIA Corporation</td>\n",
       "      <td>NasdaqGS</td>\n",
       "      <td>us</td>\n",
       "      <td>USD</td>\n",
       "      <td>-630.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.290000</td>\n",
       "      <td>129.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla, Inc.</td>\n",
       "      <td>NasdaqGS</td>\n",
       "      <td>us</td>\n",
       "      <td>USD</td>\n",
       "      <td>-130.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>303.750000</td>\n",
       "      <td>304.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-04</th>\n",
       "      <td>SHEL.L</td>\n",
       "      <td>SHELL PLC ORD EUR0.07</td>\n",
       "      <td>LSE</td>\n",
       "      <td>gb</td>\n",
       "      <td>GBp</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2314.500000</td>\n",
       "      <td>2644.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ticker                         name  exchange country currency  \\\n",
       "Date                                                                          \n",
       "2017-10-18  BTC-USD                  Bitcoin USD       CCC     ccc      USD   \n",
       "2021-05-31  ETH-USD                 Ethereum USD       CCC     ccc      USD   \n",
       "2021-06-02  BTC-USD                  Bitcoin USD       CCC     ccc      USD   \n",
       "2021-11-20  BTC-USD                  Bitcoin USD       CCC     ccc      USD   \n",
       "2021-12-03  BTC-USD                  Bitcoin USD       CCC     ccc      USD   \n",
       "2021-12-09     TSLA                  Tesla, Inc.  NasdaqGS      us      USD   \n",
       "2021-12-13     AAPL                   Apple Inc.  NasdaqGS      us      USD   \n",
       "2021-12-20     MSFT        Microsoft Corporation  NasdaqGS      us      USD   \n",
       "2021-12-27     AAPL                   Apple Inc.  NasdaqGS      us      USD   \n",
       "2021-12-27   BMW.DE  BAYERISCHE MOTOREN WERKE AG     XETRA      de      EUR   \n",
       "2021-12-27    GOOGL                Alphabet Inc.  NasdaqGS      us      USD   \n",
       "2021-12-27     MSFT        Microsoft Corporation  NasdaqGS      us      USD   \n",
       "2021-12-27     NVDA           NVIDIA Corporation  NasdaqGS      us      USD   \n",
       "2021-12-27     TSLA                  Tesla, Inc.  NasdaqGS      us      USD   \n",
       "2022-04-04   SHEL.L        SHELL PLC ORD EUR0.07       LSE      gb      GBp   \n",
       "\n",
       "            net_(eur)  operation_price  operation_price_(eur)  shares  \\\n",
       "Date                                                                    \n",
       "2017-10-18    -319.21              NaN                    NaN     NaN   \n",
       "2021-05-31    -515.80              NaN                    NaN     NaN   \n",
       "2021-06-02    -125.93              NaN                    NaN     NaN   \n",
       "2021-11-20    -146.33              NaN                    NaN     NaN   \n",
       "2021-12-03   -1084.00              NaN                    NaN     NaN   \n",
       "2021-12-09    -517.30              NaN                    NaN     NaN   \n",
       "2021-12-13    -500.00              NaN                    NaN     NaN   \n",
       "2021-12-20    -500.00              NaN                    NaN     NaN   \n",
       "2021-12-27    -131.70              NaN                    NaN     NaN   \n",
       "2021-12-27    -630.00              NaN                    NaN     NaN   \n",
       "2021-12-27    -630.00              NaN                    NaN     NaN   \n",
       "2021-12-27    -130.00              NaN                    NaN     NaN   \n",
       "2021-12-27    -630.00              NaN                    NaN     NaN   \n",
       "2021-12-27    -130.00              NaN                    NaN     NaN   \n",
       "2022-04-04    -100.00              NaN                    NaN     NaN   \n",
       "\n",
       "            current_price  current_price_(eur)  returns_(eur)  sales_num  \\\n",
       "Date                                                                       \n",
       "2017-10-18   19790.857244             19808.69            NaN          0   \n",
       "2021-05-31    1472.764744              1474.09            NaN          0   \n",
       "2021-06-02   19790.857244             19808.69            NaN          0   \n",
       "2021-11-20   19790.857244             19808.69            NaN          0   \n",
       "2021-12-03   19790.857244             19808.69            NaN          0   \n",
       "2021-12-09     303.750000               304.02            NaN          0   \n",
       "2021-12-13     152.370000               152.51            NaN          0   \n",
       "2021-12-20     245.380000               245.60            NaN          0   \n",
       "2021-12-27     152.370000               152.51            NaN          0   \n",
       "2021-12-27      74.680000                74.68            NaN          0   \n",
       "2021-12-27     102.910000               103.00            NaN          0   \n",
       "2021-12-27     245.380000               245.60            NaN          0   \n",
       "2021-12-27     129.290000               129.41            NaN          0   \n",
       "2021-12-27     303.750000               304.02            NaN          0   \n",
       "2022-04-04    2314.500000              2644.84            NaN          0   \n",
       "\n",
       "            purchases_num  \n",
       "Date                       \n",
       "2017-10-18              1  \n",
       "2021-05-31              1  \n",
       "2021-06-02              1  \n",
       "2021-11-20              1  \n",
       "2021-12-03              1  \n",
       "2021-12-09              1  \n",
       "2021-12-13              1  \n",
       "2021-12-20              1  \n",
       "2021-12-27              1  \n",
       "2021-12-27              1  \n",
       "2021-12-27              1  \n",
       "2021-12-27              1  \n",
       "2021-12-27              1  \n",
       "2021-12-27              1  \n",
       "2022-04-04              1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c9eca-81af-4ab9-af46-bbc9451f993f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a4da2-08d1-4049-8f27-7a7b8b186d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%who_ls DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f0ac7-f463-4878-bfb3-2130d585a1df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, Normalizer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, cross_validate, ShuffleSplit, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, BayesianRidge, Lasso, SGDRegressor, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f81d87-1ffd-4a47-880e-481045a3a0f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058eeab-deb8-4fcc-bce4-a9d34e0fe690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y, test_size = 0.3, ax = \"\", poly = \"\", ylim1 = None, ylim2 = None, title = \"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        * model\n",
    "        * X\n",
    "        * y\n",
    "        * train test split size (default = 0.3)\n",
    "        * ax, if you want to plot it in a combined plot (default = \"\")\n",
    "        * poly, if you want to use a polynomial model (default = \"\")\n",
    "        * ylim1 (default = None)\n",
    "        * ylim2 (default = None)\n",
    "        * title (default = \"Learning Curve\")\n",
    "    \"\"\"\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "    train_errors, test_errors = [], []\n",
    "    train_scores, test_scores = [], []\n",
    "    \n",
    "    for m in range(1, len(X_train)):\n",
    "        if poly == \"\":\n",
    "            model.fit(X_train[:m], y_train[:m])\n",
    "        else:\n",
    "            X_poly = poly.fit_transform(X_train[:m])\n",
    "            model.fit(X_poly, y_train[:m])\n",
    "            \n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_test_predict = model.predict(X_test)\n",
    "            \n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        test_errors.append(mean_squared_error(y_test, y_test_predict))\n",
    "        \n",
    "        train_scores.append(r2_score(y_train[:m], y_train_predict))\n",
    "        test_scores.append(r2_score(y_test, y_test_predict))\n",
    "        \n",
    "    if ax == \"\":\n",
    "        fig, (ax,ax2) = plt.subplots(1,2)\n",
    "        fig.suptitle(title, fontsize = 25)\n",
    "    else:\n",
    "        ax.title(title, fontsize = 25, pad = 20)\n",
    "        ax2 = plt.twinx()\n",
    "        ax2.grid(False)\n",
    "\n",
    "    ax.plot(np.sqrt(train_errors), \"r-\", linewidth=2, label=\"Train mse\")\n",
    "    ax.plot(np.sqrt(test_errors), \"b-\", linewidth=3, label=\"Test mse\")\n",
    "    ax.set_ylim(ylim1,ylim2)\n",
    "    ax.legend(fontsize = 15, loc = \"upper left\")\n",
    "    ax.set_ylabel(\"MSE\", fontsize = 12)\n",
    "    \n",
    "    ax2.plot(train_scores, c = \"coral\", linewidth=2, label=\"Train r2 score\")\n",
    "    ax2.plot(test_scores, c = \"skyblue\", linewidth=3, label=\"Test r2 score\")\n",
    "    ax2.legend(fontsize = 15, loc = \"lower right\")\n",
    "    ax2.set_ylim(-0.1,1.1)\n",
    "    ax2.set_ylabel(\"r2 score\", fontsize = 12)\n",
    "\n",
    "def grouped_barplot_stat(bars1, bars2, ylim1 = 0.5, barWidth = 0.4, title = \"Grouped Barplot\"):\n",
    "    \"\"\"\n",
    "    Grouped bar chart with two categoricals\n",
    "    Input:\n",
    "        * bars1 data (array/list)\n",
    "        * bars2 data (array/list)\n",
    "        * ylim1 min (default = 0.5)\n",
    "        * bar width (default = 0.4)\n",
    "        * title (default = \"Grouped Barplot\")\n",
    "    \"\"\"\n",
    "    # width of the bars\n",
    "    barWidth = barWidth\n",
    "    \n",
    "    bars1 = bars1\n",
    "    bars2 = bars2\n",
    "     \n",
    "    # Choose the height of the error bars\n",
    "    # yer1 = [0] * len(bars1)\n",
    "    # yer2 = [0] * len(bars2)\n",
    "     \n",
    "    # The x position of bars\n",
    "    r1 = np.arange(len(bars1))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "     \n",
    "    plt.bar(r1, bars1, width = barWidth, color = 'mediumturquoise', alpha = 0.7, edgecolor = 'white', capsize=7, label='Train') #, yerr=yer1\n",
    "    plt.bar(r2, bars2, width = barWidth, color = 'teal', alpha = 0.7, edgecolor = 'white', capsize=7, label='Test')\n",
    "    \n",
    "    med = np.zeros(0)\n",
    "    for x, y in zip(bars1,bars2):\n",
    "        med = np.append(med,np.mean([x,y]))\n",
    "    plt.plot(med, linewidth = 3, c = \"lightcoral\", alpha = 0.85)\n",
    "    \n",
    "    # general layout\n",
    "    plt.title(title, fontsize = 25, pad = 20)\n",
    "    plt.ylim(ylim1,1.01)\n",
    "    plt.ylabel('r2 score', fontsize = 12)\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    \n",
    "    for i, e in enumerate(bars2):\n",
    "        plt.annotate(np.round(e,2), xy = (i -barWidth/2, e+0.01), fontsize = 12, alpha = 0.7)\n",
    "    \n",
    "    for i, e in enumerate(bars1):\n",
    "        plt.annotate(np.round(e,2), xy = (i +barWidth/2, e+0.01), fontsize = 12, alpha = 0.7)\n",
    "    \n",
    "    table = plt.table(cellText = np.array([[np.round(np.mean(bars1),2),np.round(np.std(bars1),3)],[np.round(np.mean(bars2),2),np.round(np.std(bars2),3)]]),\n",
    "                      colLabels = [\"Mean\", \"Standard Desv\"], \n",
    "                      bbox=[0.35, -0.22, 0.25, 0.2],\n",
    "                      rowLabels = [\"Train\", \"Test\"], \n",
    "                      cellLoc = \"center\", colLoc = \"center\", loc = \"center\",\n",
    "                     cellColours = [[\"#EAEAF2\",\"#EAEAF2\"],\n",
    "                                        [\"#EAEAF2\",\"#EAEAF2\"]],\n",
    "                         rowColours = [\"#EAEAF2\",\"#EAEAF2\"],\n",
    "                         colColours = [\"#EAEAF2\",'#EAEAF2'])\n",
    "    \n",
    "    # table.scale(1,2)\n",
    "    table.set_fontsize(16)\n",
    "    \n",
    "    plt.legend(loc = \"lower left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def kbest(df, target_tic_stock, S, k=100):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        * DataFrame from where to get train data\n",
    "        * target tic stock (ex. (\"AAPL\", \"Close\"))\n",
    "        * k; number of top features to select (default = 100)\n",
    "        * RETURNS: featureScores and y\n",
    "    \"\"\"\n",
    "    # X = df.iloc[:,np.where(df.columns != target_tic_stock)[0]]\n",
    "    # y = df[target_tic_stock].values\n",
    "    X = df.iloc[:,np.where(df.columns != target_tic_stock)[0]].iloc[:-S].dropna()  # Correlated Features truncated S steps in the end\n",
    "    y = df[target_tic_stock].shift(S).dropna().values # Target shifted S steps\n",
    "\n",
    "    bestfeatures = SelectKBest(score_func=f_regression, k=k)\n",
    "    fit = bestfeatures.fit(X,y)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X.columns)\n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']\n",
    "   \n",
    "    return featureScores, y\n",
    "\n",
    "\n",
    "def reg_hyper_params(model, target_tic, X, y, datain, tscv,ts):\n",
    "    global results_grid\n",
    "\n",
    "    if model == 'bayridge_reg':\n",
    "        estimator = BayesianRidge()\n",
    "        alpha_init = [0.01, 0.1, 1, 10]\n",
    "        lambda_init = [0.01, 0.1, 1, 10]\n",
    "        param_grid = dict(alpha_init=alpha_init, lambda_init=lambda_init)        \n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "\n",
    "    elif model == 'ridge_reg':\n",
    "        estimator = Ridge()\n",
    "        alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "        param_grid = dict(alpha=alpha)\n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "    \n",
    "    elif model == 'lasso_reg':\n",
    "        estimator = Lasso()\n",
    "        alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "        param_grid = dict(alpha = alpha)\n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "    \n",
    "    elif model == 'elastic_reg':\n",
    "        estimator = ElasticNet()\n",
    "        alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "        l1_ratio = [np.round(0.2*y,1) for y in range(0,6)]\n",
    "        param_grid = dict(alpha = alpha, l1_ratio = l1_ratio)\n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "    \n",
    "    elif model == 'svm_reg':\n",
    "        estimator = SVR()\n",
    "        kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "        C = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "        degree = [2,3]\n",
    "        param_grid = dict(kernel = kernel, C = C, degree = degree)\n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "    \n",
    "    elif model == 'sdg_reg':\n",
    "        estimator = SGDRegressor()\n",
    "        alpha = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "        l1_ratio = [np.round(0.2*y,1) for y in range(0,6)]\n",
    "        learning_rate = [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"]\n",
    "        eta0 = [0.001, 0.01, 0.1, 1, 10]\n",
    "        param_grid = dict(alpha = alpha, l1_ratio = l1_ratio, learning_rate = learning_rate, eta0 = eta0)\n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "    \n",
    "    elif model == 'tree_reg':\n",
    "        estimator = DecisionTreeRegressor()\n",
    "        max_depth = [y for y in range(1,16)]\n",
    "        param_grid = dict(max_depth = max_depth)\n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "    \n",
    "    elif model == 'ranforest_reg':\n",
    "        estimator = RandomForestRegressor(n_jobs = -1)\n",
    "        max_depth = [y for y in range(1,16)]\n",
    "        param_grid = dict(max_depth = max_depth)\n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "\n",
    "    elif model == 'poly_reg':\n",
    "        estimator = make_pipeline(PolynomialFeatures(), LinearRegression(n_jobs = -1))\n",
    "        param_grid = {'polynomialfeatures__degree': range(2,4)}\n",
    "                      # 'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False]}\n",
    "        grid = GridSearchCV(estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_\n",
    "        return grid.fit(X,y)\n",
    "    \n",
    "    elif model == 'knei_reg':\n",
    "        estimator = KNeighborsRegressor(n_jobs = -1)\n",
    "        n_neighbors = [y for y in range(1,20)]\n",
    "        param_grid = dict(n_neighbors = n_neighbors)\n",
    "        grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", n_jobs = -1, cv = tscv, return_train_score = True)\n",
    "        results_grid[(target_tic,model,datain,ts)] = grid.fit(X,y).best_params_        \n",
    "        return grid.fit(X,y)\n",
    "    \n",
    "    else:\n",
    "        print(\"No regression model selected\")\n",
    "\n",
    "def test_size_opt_reg(X, y, target_tic, model, datain, n_features):\n",
    "    global results_reg_df\n",
    "    \n",
    "    test_size_porc = [np.round(0.05*x,2) for x in range(1,8)]\n",
    "    for ts in test_size_porc:\n",
    "        n_splits = int(np.trunc(X.shape[0]/int(X.shape[0]*ts)))\n",
    "        test_size = int(X.shape[0]*ts)\n",
    "        tscv = TimeSeriesSplit(n_splits = n_splits, gap = 0, test_size = test_size-1)\n",
    "        \n",
    "        if model == \"lin_reg\":\n",
    "            estimator = LinearRegression()\n",
    "            ind = X.shape[0]-int(X.shape[0]*ts)\n",
    "            X_train, X_test = X[:ind,:], X[ind:,:]\n",
    "            y_train, y_test = y[:ind], y[ind:]\n",
    "            estimator.fit(X_train,y_train)\n",
    "            results_reg_df.loc[(target_tic,model,datain, n_features, ts, n_splits),:] = (np.round(r2_score(y_train,estimator.predict(X_train)),2),\n",
    "                                                                           0,\n",
    "                                                                           0,\n",
    "                                                                           np.round(r2_score(y_test,estimator.predict(X_test)),2),\n",
    "                                                                           0,\n",
    "                                                                           0)             \n",
    "        else:    \n",
    "            grid_fit = reg_hyper_params(model, target_tic, X, y, datain, tscv,ts)\n",
    "            results_reg_df.loc[(target_tic,model,datain, n_features, ts, n_splits),:] = (np.round(grid_fit.cv_results_[f'split{n_splits-1}_train_score'][grid_fit.best_index_],2),\n",
    "                                                                           np.round(grid_fit.cv_results_['mean_train_score'][grid_fit.best_index_],2),\n",
    "                                                                           np.round(grid_fit.cv_results_['std_train_score'][grid_fit.best_index_],2),\n",
    "                                                                           np.round(grid_fit.cv_results_[f'split{n_splits-1}_test_score'][grid_fit.best_index_],2),\n",
    "                                                                           np.round(grid_fit.cv_results_['mean_test_score'][grid_fit.best_index_],2),\n",
    "                                                                           np.round(grid_fit.cv_results_['std_test_score'][grid_fit.best_index_],2))                                                                    \n",
    "    return results_reg_df\n",
    "\n",
    "def reg_estimator(model, target_tic, X, y, datain, results_grid,ts,sc):\n",
    "\n",
    "    if model == 'lin_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, LinearRegression())\n",
    "        else:\n",
    "            return LinearRegression()\n",
    "        \n",
    "    elif model == 'bayridge_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, BayesianRidge())\n",
    "        else:\n",
    "            return BayesianRidge()        \n",
    "\n",
    "    elif model == 'ridge_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, Ridge(alpha = results_grid[(target_tic,model,datain,ts)][\"alpha\"]))\n",
    "        else:\n",
    "            return Ridge(alpha = results_grid[(target_tic,model,datain,ts)][\"alpha\"])\n",
    "    \n",
    "    elif model == 'lasso_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, Lasso(alpha = results_grid[(target_tic,model,datain,ts)][\"alpha\"]))\n",
    "        else:\n",
    "            return Lasso(alpha = results_grid[(target_tic,model,datain,ts)][\"alpha\"])\n",
    "    \n",
    "    elif model == 'elastic_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, ElasticNet(alpha = results_grid[(target_tic,model,datain,ts)][\"alpha\"], l1_ratio = results_grid[(target_tic,model,datain,ts)][\"l1_ratio\"]))\n",
    "        else:\n",
    "            return ElasticNet(alpha = results_grid[(target_tic,model,datain,ts)][\"alpha\"], l1_ratio = results_grid[(target_tic,model,datain,ts)][\"l1_ratio\"])       \n",
    "    \n",
    "    elif model == 'svm_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, SVR(kernel = results_grid[(target_tic,model,datain,ts)][\"kernel\"], \n",
    "                      degree = results_grid[(target_tic,model,datain,ts)][\"degree\"],\n",
    "                     C = results_grid[(target_tic,model,datain,ts)][\"C\"]))\n",
    "        else:\n",
    "            return SVR(kernel = results_grid[(target_tic,model,datain,ts)][\"kernel\"], \n",
    "                      degree = results_grid[(target_tic,model,datain,ts)][\"degree\"],\n",
    "                     C = results_grid[(target_tic,model,datain,ts)][\"C\"])\n",
    "    \n",
    "    elif model == 'sdg_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, SGDRegressor(alpha = results_grid[(target_tic,model,datain,ts)][\"alpha\"], l1_ratio = results_grid[(target_tic,model,datain,ts)][\"l1_ratio\"],\n",
    "                              learning_rate = results_grid[(target_tic,model,datain,ts)][\"learning_rate\"], eta0 = results_grid[(target_tic,model,datain,ts)][\"eta0\"]))\n",
    "        else:       \n",
    "            return SGDRegressor(alpha = results_grid[(target_tic,model,datain,ts)][\"alpha\"], l1_ratio = results_grid[(target_tic,model,datain,ts)][\"l1_ratio\"],\n",
    "                              learning_rate = results_grid[(target_tic,model,datain,ts)][\"learning_rate\"], eta0 = results_grid[(target_tic,model,datain,ts)][\"eta0\"])\n",
    "    \n",
    "    elif model == 'tree_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, DecisionTreeRegressor(max_depth = results_grid[(target_tic,model,datain,ts)][\"max_depth\"]))\n",
    "        else:\n",
    "            return DecisionTreeRegressor(max_depth = results_grid[(target_tic,model,datain,ts)][\"max_depth\"])\n",
    "    \n",
    "    elif model == 'ranforest_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, RandomForestRegressor(max_depth = results_grid[(target_tic,model,datain,ts)][\"max_depth\"], n_jobs = -1))\n",
    "        else:\n",
    "            return RandomForestRegressor(max_depth = results_grid[(target_tic,model,datain,ts)][\"max_depth\"], n_jobs = -1)\n",
    "    \n",
    "    elif model == 'poly_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, PolynomialFeatures(degree = results_grid[(target_tic,model,datain,ts)][\"polynomialfeatures__degree\"]), LinearRegression())\n",
    "        else:\n",
    "            return make_pipeline(PolynomialFeatures(degree = results_grid[(target_tic,model,datain,ts)][\"polynomialfeatures__degree\"]), LinearRegression())\n",
    "    \n",
    "    elif model == 'knei_reg':\n",
    "        if sc != \"nosc\":\n",
    "            return make_pipeline(sc, KNeighborsRegressor(n_neighbors = results_grid[(target_tic,model,datain,ts)][\"n_neighbors\"], n_jobs = -1))\n",
    "        else:\n",
    "            return KNeighborsRegressor(n_neighbors = results_grid[(target_tic,model,datain,ts)][\"n_neighbors\"], n_jobs = -1)\n",
    "    \n",
    "    else:\n",
    "        print(\"No regression model selected\")\n",
    "\n",
    "\n",
    "def clf_hyper_params(model, target_tic, X, y, datain):\n",
    "    if model == \"\":\n",
    "        pass\n",
    "    else:\n",
    "        print(\"No classification model selected\")      \n",
    "\n",
    "def test_size_opt_clf(estimator, X, y, target_tic, model, datain, n_features):\n",
    "    global results_reg_df\n",
    "    \n",
    "    test_size_porc = [np.round(0.05*x,2) for x in range(1,8)]\n",
    "    for ts in test_size_porc:\n",
    "        n_splits = int(np.trunc(X.shape[0]/int(X.shape[0]*ts)))\n",
    "        test_size = int(X.shape[0]*ts)\n",
    "        tscv = TimeSeriesSplit(n_splits = n_splits, gap = 0, test_size = test_size-1)\n",
    "        \n",
    "        grid_fit = clf_hyper_params(model, target_tic, X, y, datain, tscv)\n",
    "        results_reg_df.loc[(target_tic,model,datain, n_features, ts, n_splits),:] = (np.round(grid_fit.cv_results_[f'split{n_splits-1}_train_score'][grid_fit.best_index_],2),\n",
    "                                                                       np.round(grid_fit.cv_results_['mean_train_score'][grid_fit.best_index_],2),\n",
    "                                                                       np.round(grid_fit.cv_results_['std_train_score'][grid_fit.best_index_],2),\n",
    "                                                                       np.round(grid_fit.cv_results_[f'split{n_splits-1}_test_score'][grid_fit.best_index_],2),\n",
    "                                                                       np.round(grid_fit.cv_results_['mean_test_score'][grid_fit.best_index_],2),\n",
    "                                                                       np.round(grid_fit.cv_results_['std_test_score'][grid_fit.best_index_],2))                                                                    \n",
    "    return results_reg_df\n",
    "\n",
    "def prog_bar2(part, collection, label = \"Processing\", it = \"\", labelwidth = \"20%\", barwidth = \"25%\", barcolor = \"purple\"):\n",
    "    \"\"\"\n",
    "    Purple progress bar\n",
    "    Input:\n",
    "        * The code has two parts, one before and the other after the loop.\n",
    "        * The collection needed to make proportion of progress, from 0 to 100%.\n",
    "        * Label of the bar as a string. Example: \"Processing Stats\"\n",
    "        * Iteration element from collection\n",
    "        * Label width in string percentage. Ex. \"20%\"\n",
    "        * Bar width in string percentage. Ex. \"25%\"\n",
    "        * Bar Color. Ex. \"Purple\"\n",
    "    \"\"\"\n",
    "    global c\n",
    "    global progress_bar\n",
    "    if part == 1:\n",
    "        c = 0\n",
    "        label = widgets.Label(f'{label} --> ', layout=widgets.Layout(width = labelwidth))\n",
    "        try:\n",
    "            progress_bar = IntProgress(min=0,max=collection, style = {'description_width': 'initial'}, \n",
    "                                   layout=widgets.Layout(width = barwidth))\n",
    "        except:\n",
    "            progress_bar = IntProgress(min=0,max=len(collection), style = {'description_width': 'initial'}, \n",
    "                                   layout=widgets.Layout(width = barwidth))\n",
    "        progress_bar.style.bar_color = barcolor\n",
    "        return display(widgets.Box(children=[label,progress_bar]))\n",
    "    elif part == 2:\n",
    "        c += 1\n",
    "        progress_bar.value += 1\n",
    "        progress_bar.description = f\"{it}\"\n",
    "    else:\n",
    "        print(\"There is an error with the indicated part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1115b4-5d9d-4ede-9d8a-63ca4904c875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".p-Widget.jp-RenderedImage.jp-mod-trusted.jp-OutputArea-output {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af257b5-c137-4729-80eb-68c928a13ed8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9cec2-8f4a-47b6-a179-ded5dda3f5e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1. Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfd648-7e6e-49e1-ac21-e359ecadb1c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1.1. Algortihmic Regression Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0de12d6b-6bc3-4763-ba69-64592cfa67d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "results_grid = dict() # Grid Search results\n",
    "\n",
    "# Cross Validation results and Timeseries split optimization results\n",
    "results_reg_df = pd.DataFrame(index = pd.MultiIndex.from_arrays([[\"ticker\"],[\"model\"],[\"datain\"],[\"n_features\"],[\"test_size_prop\"], [\"n_splits\"]]), \n",
    "                    columns = pd.MultiIndex.from_product([[\"train\",\"test\"], [\"score\", \"mean_scorecv\",\"std_scorecv\"]])).iloc[1:,:]\n",
    "results_reg_df.index.names = [\"ticker\",\"model\",\"datain\",\"n_features\", \"test_size_prop\", \"n_splits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00aaf9-b552-499d-a70c-20b3f5055360",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HEADER\n",
    "# models = np.array(['lin_reg', 'ridge_reg', 'lasso_reg', 'elastic_reg', 'svm_reg', \\\n",
    "#          'sdg_reg', 'bayridge_reg', 'tree_reg', 'ranforest_reg', 'poly_reg', 'knei_reg'])\n",
    "models = np.array(['lin_reg', 'ridge_reg', 'elastic_reg', 'bayridge_reg'])\n",
    "# models = np.array(['lin_reg', 'ridge_reg'])\n",
    "## DATA\n",
    "df = data2y_ret\n",
    "datain = \"rets\"\n",
    "\n",
    "## MODEL SELECTION\n",
    "corr_selection = True\n",
    "ncoef = np.array([np.round((x+0.05),2) for x in np.arange(0.5,0.85,0.05)])\n",
    "matcorr = matcorr_data2y_ret\n",
    "\n",
    "## DATA SHIFTING TO PREDICT FUTURE WITH CURRENT FEATURES VALUES\n",
    "S = 10\n",
    "\n",
    "kbest_selection = False\n",
    "nkbest = 4\n",
    "pca = False\n",
    "\n",
    "scaling = True\n",
    "# scalers = [StandardScaler(), MinMaxScaler(), Normalizer(),\"nosc\"]\n",
    "scalers = [StandardScaler(),\"nosc\"]\n",
    "\n",
    "## ML ALGORITHM\n",
    "regression = True\n",
    "scoring = [\"r2\"]\n",
    "\n",
    "classification = False"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52c88174-b1b5-469b-87d2-446300e65c5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "now1 = datetime.now()\n",
    "\n",
    "prog_bar2(1, ticks_strategy.size*models.size*ncoef.size*len(scalers), label = \"Processing ML Optimization\", it = \"\", labelwidth = \"18%\", barwidth = \"25%\", barcolor = \"purple\")\n",
    "\n",
    "# ['AAPL', 'BMW.DE', 'BTC-USD', 'ETH-USD', 'GOOGL', 'MSFT', 'NVDA','SHEL.L', 'TSLA']\n",
    "for target_tic in ticks_strategy:\n",
    "# for target_tic in ['TSLA']:\n",
    "    target_tic_stock = (target_tic,\"Close\")\n",
    "    \n",
    "    for model in models:\n",
    "        if corr_selection:\n",
    "            flag_kbest = True\n",
    "            for coef in ncoef:\n",
    "                df_corr_coef = matcorr[target_tic][(matcorr[target_tic] > coef) | (matcorr[target_tic] < -coef)][\"Close\"].dropna(how=\"all\")\n",
    "                ticks_corr = df_corr_coef.index.unique()\n",
    "                ticks_corr = np.delete(ticks_corr,np.where(ticks_corr == target_tic_stock))\n",
    "                if ticks_corr.size > 5:\n",
    "                    flag_kbest = False # IF THE TICKER GETS IN AT LEAST 1 TIME, WILL NOT GET INSIDE KBEST\n",
    "                    prog_bar2(2, ticks_strategy.size*models.size*ncoef.size*len(scalers), it = f\"{target_tic} | {model.upper()} | {coef}\")\n",
    "                    # X = df[ticks_corr].values\n",
    "                    # y = df[target_tic_stock].values\n",
    "                    X = df[ticks_corr].iloc[:-S].dropna().values  # Correlated Features truncated S steps in the end\n",
    "                    y = df[target_tic_stock].shift(S).dropna().values # Target shifted S steps\n",
    "                    \n",
    "                    if datain.find(\"_corr\") == -1:\n",
    "                        datain = datain+\"_corr\"+f\"{coef}\"\n",
    "                    else:\n",
    "                        datain = datain[:datain.find(\"_corr\")]+\"_corr\"+f\"{coef}\"\n",
    "                    n_features = X.shape[1]\n",
    "                    ## DATA SCALING    \n",
    "                    if scaling:\n",
    "                        for scaler in scalers:\n",
    "                            # prog_bar(2, ticks_strategy.size*models.size*ncoef.size*len(scalers), it = f\"{target_tic} | {model.upper()} | {coef}\")\n",
    "                            if scaler != \"nosc\":\n",
    "                                X_sc = scaler.fit_transform(X)                              \n",
    "                                sc_var = \"_sc\"+str(scaler)[:4]\n",
    "                                if datain.find(\"_sc\") == -1 and datain.find(\"_nosc\") == -1:\n",
    "                                    datain = datain+sc_var\n",
    "                                elif datain.find(\"_sc\") == -1 and datain.find(\"_nosc\") != -1:\n",
    "                                    datain = datain[:datain.find(\"_nosc\")]+sc_var\n",
    "                                else:\n",
    "                                    datain = datain[:datain.find(\"_sc\")]+sc_var\n",
    "    \n",
    "                                ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                                if regression:\n",
    "                                    results_reg_df = test_size_opt_reg(X_sc, y, target_tic, model, datain, n_features)\n",
    "        \n",
    "                                if classification:\n",
    "                                    results_reg_df = test_size_opt_clf(X_sc, y, target_tic, model, datain, n_features)\n",
    "\n",
    "                            else:\n",
    "                                # prog_bar(2, ticks_strategy.size*models.size*ncoef.size, it = f\"{target_tic} | {model.upper()} | {coef}\")\n",
    "                                sc_var = \"_nosc\"\n",
    "                                if datain.find(\"_nosc\") == -1 and datain.find(\"_sc\") == -1:\n",
    "                                    datain = datain+sc_var       \n",
    "                                elif datain.find(\"_sc\") != -1 and datain.find(\"_nosc\") == -1:\n",
    "                                    datain = datain[:datain.find(\"_sc\")]+sc_var      \n",
    "                                else:\n",
    "                                    datain = datain[:datain.find(\"_nosc\")]+sc_var\n",
    "                                    \n",
    "                                ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                                if regression:\n",
    "                                    results_reg_df = test_size_opt_reg(X, y, target_tic, model, datain, n_features)\n",
    "        \n",
    "                                if classification:\n",
    "                                    results_reg_df = test_size_opt_clf(X, y, target_tic, model, datain, n_features)\n",
    "\n",
    "                    else:\n",
    "                        # prog_bar(2, ticks_strategy.size*models.size*ncoef.size, it = f\"{target_tic} | {model.upper()} | {coef}\")\n",
    "                        ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                        if regression:\n",
    "                            results_reg_df = test_size_opt_reg(X, y, target_tic, model, datain, n_features)\n",
    "\n",
    "                        if classification:\n",
    "                            results_reg_df = test_size_opt_clf(X, y, target_tic, model, datain, n_features) \n",
    "            \n",
    "            if flag_kbest:\n",
    "                print(f\"{target_tic} in kbest\")\n",
    "                featureScores, y = kbest(df, target_tic_stock, S, 100)     \n",
    "                for n in np.array([np.int(featureScores.shape[0]*.01)*i for i in range(1,nkbest)]):\n",
    "                    X = df[featureScores.nlargest(n,'Score').Specs.values].iloc[:-S].dropna().values  # Correlated Features truncated S steps in the end\n",
    "                    # X = df[featureScores.nlargest(n,'Score').Specs.values].values\n",
    "                    n_features = X.shape[1]\n",
    "                    \n",
    "                    if datain.find(\"_kbest\") == -1 and datain.find(\"_corr\") == -1:\n",
    "                        datain = datain+\"_kbest\"+f\"{X.shape[1]}\"\n",
    "                    elif datain.find(\"_corr\") != -1:\n",
    "                        datain = datain[:datain.find(\"_corr\")]+\"_kbest\"+f\"{X.shape[1]}\"\n",
    "                    else:\n",
    "                        datain = datain[:datain.find(\"_kbest\")]+\"_kbest\"+f\"{X.shape[1]}\"\n",
    "                        \n",
    "                    if scaling:\n",
    "                        for scaler in scalers:\n",
    "                            prog_bar2(2, ticks_strategy.size*models.size*nkbest*len(scalers), it = f\"{target_tic} | {model.upper()} | {n}\")\n",
    "                            if scaler != \"nosc\":\n",
    "                                X_sc = scaler.fit_transform(X)                              \n",
    "                                sc_var = \"_sc\"+str(scaler)[:4]\n",
    "                                if datain.find(\"_sc\") == -1 and datain.find(\"_nosc\") == -1:\n",
    "                                    datain = datain+sc_var\n",
    "                                elif datain.find(\"_sc\") == -1 and datain.find(\"_nosc\") != -1:\n",
    "                                    datain = datain[:datain.find(\"_nosc\")]+sc_var\n",
    "                                else:\n",
    "                                    datain = datain[:datain.find(\"_sc\")]+sc_var\n",
    "    \n",
    "                                ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                                if regression:\n",
    "                                    results_reg_df = test_size_opt_reg(X_sc, y, target_tic, model, datain, n_features)\n",
    "        \n",
    "                                if classification:\n",
    "                                    results_reg_df = test_size_opt_clf(X_sc, y, target_tic, model, datain, n_features)\n",
    "                            else:\n",
    "                                prog_bar2(2, ticks_strategy.size*models.size*nkbest*len(scalers), it = f\"{target_tic} | {model.upper()} | {n}\")\n",
    "                                sc_var = \"_nosc\"\n",
    "                                if datain.find(\"_nosc\") == -1 and datain.find(\"_sc\") == -1:\n",
    "                                    datain = datain+sc_var       \n",
    "                                elif datain.find(\"_sc\") != -1 and datain.find(\"_nosc\") == -1:\n",
    "                                    datain = datain[:datain.find(\"_sc\")]+sc_var      \n",
    "                                else:\n",
    "                                    datain = datain[:datain.find(\"_nosc\")]+sc_var\n",
    "                                    \n",
    "                                ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                                if regression:\n",
    "                                    results_reg_df = test_size_opt_reg(X, y, target_tic, model, datain, n_features)\n",
    "        \n",
    "                                if classification:\n",
    "                                    results_reg_df = test_size_opt_clf(X, y, target_tic, model, datain, n_features)\n",
    "\n",
    "                    else:\n",
    "                        prog_bar2(2, ticks_strategy.size*models.size*nkbest, it = f\"{target_tic} | {model.upper()} | {n}\")\n",
    "                        ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                        if regression:\n",
    "                            results_reg_df = test_size_opt_reg(X, y, target_tic, model, datain, n_features)\n",
    "                        if classification:\n",
    "                            results_reg_df = test_size_opt_clf(X, y, target_tic, model, datain, n_features) \n",
    "\n",
    "        elif kbest_selection:\n",
    "            featureScores, y = kbest(df, target_tic_stock,S, 100)     \n",
    "            for n in np.array([np.int(featureScores.shape[0]*.01)*i for i in range(1,nkbest)]):\n",
    "                X = df[featureScores.nlargest(n,'Score').Specs.values].iloc[:-S].dropna().values  # Correlated Features truncated S steps in the end\n",
    "                # X = df[featureScores.nlargest(n,'Score').Specs.values].values\n",
    "                n_features = X.shape[1]\n",
    "\n",
    "                if datain.find(\"_kbest\") == -1 and datain.find(\"_corr\") == -1:\n",
    "                    datain = datain+\"_kbest\"+f\"{X.shape[1]}\"\n",
    "                elif datain.find(\"_corr\") != -1:\n",
    "                    datain = datain[:datain.find(\"_corr\")]+\"_kbest\"+f\"{X.shape[1]}\"\n",
    "                else:\n",
    "                    datain = datain[:datain.find(\"_kbest\")]+\"_kbest\"+f\"{X.shape[1]}\"\n",
    "            \n",
    "                ## DATA SCALING    \n",
    "                if scaling:\n",
    "                    for scaler in scalers:\n",
    "                        prog_bar2(2, ticks_strategy.size*models.size*nkbest*len(scalers), it = f\"{target_tic} | {model.upper()} | {n}\")\n",
    "                        if scaler != \"nosc\":\n",
    "                            X_sc = scaler.fit_transform(X)                              \n",
    "                            sc_var = \"_sc\"+str(scaler)[:4]\n",
    "                            if datain.find(\"_sc\") == -1 and datain.find(\"_nosc\") == -1:\n",
    "                                datain = datain+sc_var\n",
    "                            elif datain.find(\"_sc\") == -1 and datain.find(\"_nosc\") != -1:\n",
    "                                datain = datain[:datain.find(\"_nosc\")]+sc_var\n",
    "                            else:\n",
    "                                datain = datain[:datain.find(\"_sc\")]+sc_var\n",
    "                        \n",
    "                            ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                            if regression:\n",
    "                                results_reg_df = test_size_opt_reg(X_sc, y, target_tic, model, datain, n_features)\n",
    "    \n",
    "                            if classification:\n",
    "                                results_reg_df = test_size_opt_clf(X_sc, y, target_tic, model, datain, n_features)\n",
    "                        else:\n",
    "                            prog_bar2(2, ticks_strategy.size*models.size*nkbest*len(scalers), it = f\"{target_tic} | {model.upper()} | {n}\")\n",
    "                            if datain.find(\"_nosc\") == -1 and datain.find(\"_sc\") == -1:\n",
    "                                datain = datain+\"_nosc\"        \n",
    "                            elif datain.find(\"_sc\") != -1 and datain.find(\"_nosc\") == -1:\n",
    "                                datain = datain[:datain.find(\"_sc\")]+\"_nosc\"       \n",
    "                            else:\n",
    "                                datain = datain[:datain.find(\"_nosc\")]+\"_nosc\"\n",
    "                                \n",
    "                            ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                            if regression:\n",
    "                                results_reg_df = test_size_opt_reg(X, y, target_tic, model, datain, n_features)\n",
    "    \n",
    "                            if classification:\n",
    "                                results_reg_df = test_size_opt_clf(X, y, target_tic, model, datain, n_features)\n",
    "                else:\n",
    "                    prog_bar2(2, ticks_strategy.size*models.size*nkbest, it = f\"{target_tic} | {model.upper()} | {n}\")\n",
    "                    if datain.find(\"_nosc\") == -1 and datain.find(\"_sc\") == -1:\n",
    "                        datain = datain+\"_nosc\"        \n",
    "                    elif datain.find(\"_sc\") != -1 and datain.find(\"_nosc\") == -1:\n",
    "                        datain = datain[:datain.find(\"_sc\")]+\"_nosc\"       \n",
    "                    else:\n",
    "                        datain = datain[:datain.find(\"_nosc\")]+\"_nosc\"\n",
    "    \n",
    "                    ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH. SCORES RESULTS DF\n",
    "                    if regression:\n",
    "                        results_reg_df = test_size_opt_reg(X, y, target_tic, model, datain, n_features)\n",
    "                    if classification:\n",
    "                        results_reg_df = test_size_opt_clf(X, y, target_tic, model, datain, n_features)\n",
    "                \n",
    "        elif pca:\n",
    "            # n_features = X.shape[1]\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            print(\"Set selection method\")\n",
    "\n",
    "best_models1 = results_reg_df.copy().astype(float)\n",
    "best_models1.columns = results_reg_df.columns.map('_'.join)\n",
    "best_models1 = best_models1.loc[best_models1.groupby([\"ticker\"])['test_score'].idxmax().values,:]\n",
    "to_improve = best_models1[best_models1[\"test_score\"] < 0.86]\n",
    "best_models = best_models1[best_models1.index.isin(to_improve.index) == False]\n",
    "best_models.columns = pd.MultiIndex.from_product([[\"train\",\"test\"], [\"score\", \"mean_scorecv\",\"std_scorecv\"]])\n",
    "\n",
    "del sc_var\n",
    "\n",
    "now2 = datetime.now()\n",
    "delta = now2 - now1\n",
    "print(f\"ML Optimization Done! - Process time: {delta.seconds} seconds ({np.round(delta.seconds / 60,2)} minutes)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89ba3736-f07d-4262-b016-627bb806f69a",
   "metadata": {},
   "source": [
    "best_models1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66e007a6-c13f-4b92-9b06-ba98abf43cfd",
   "metadata": {},
   "source": [
    "results_reg_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e143ba4-9085-4783-b9fb-aac877bdc98d",
   "metadata": {},
   "source": [
    "to_improve"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a6712ab-f75a-4cf5-b839-6ef65048971f",
   "metadata": {},
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a212d-f06d-40b2-8618-0d9ee69b55e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1.2. Export / Import Algortihmic Regression Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88d4db-a167-4f87-813f-1e89352cb4c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 4.1.2.1. HDFS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b47db962-d814-408c-8eef-99ad27267315",
   "metadata": {
    "tags": []
   },
   "source": [
    "##EXPORT\n",
    "### HDFS\n",
    "# results_reg_df.to_hdf(\"export/ML_algo_reg_res/Alg_reg_mod_res.h5\",results_reg_df)\n",
    "\n",
    "store = pd.HDFStore(\"export/ML_algo_reg_res/Alg_reg_mod_res.h5\", \"a\")\n",
    "store.put(\"results_reg_df\", results_reg_df)\n",
    "store.put(\"best_models\",best_models)\n",
    "store.put(\"to_improve\",to_improve)\n",
    "results_grid2 = pd.DataFrame.from_dict(results_grid,orient=\"index\")\n",
    "store.put(\"results_grid\",results_grid2)\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e63ebcfb-0380-4045-a357-768436387b3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##IMPORT\n",
    "# store = pd.HDFStore(f\"export/ML_algo_reg_res/Alg_reg_mod_res.h5\", \"r\")\n",
    "# results_grid2 = store.select(\"results_grid\")\n",
    "# store.close()\n",
    "\n",
    "##INFO\n",
    "# store = pd.HDFStore(f\"export/ML_algo_reg_res/Alg_reg_mod_res.h5\")\n",
    "# print(store.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09e5d3-e592-41f7-b360-0a117281a0fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 4.1.2.2. PARQUET"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22e4d8f6-45ab-4082-985e-5c6dd7765b98",
   "metadata": {
    "tags": []
   },
   "source": [
    "##EXPORT PARQUET\n",
    "\n",
    "results_grid2 = pd.DataFrame.from_dict(results_grid,orient=\"index\")\n",
    "results_reg_df.to_parquet(\"export/ML_algo_reg_res/results_reg_df.gzip\", compression = \"gzip\")\n",
    "best_models.to_parquet(\"export/ML_algo_reg_res/best_models.gzip\", compression = \"gzip\")\n",
    "best_models1.to_parquet(\"export/ML_algo_reg_res/best_models1.gzip\", compression = \"gzip\") \n",
    "to_improve.to_parquet(\"export/ML_algo_reg_res/to_improve.gzip\", compression = \"gzip\")\n",
    "results_grid2.to_parquet(\"export/ML_algo_reg_res/results_grid2.gzip\", compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45c18d-423f-49e8-9e14-12414225ffd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##IMPORT PARQUET\n",
    "\n",
    "results_reg_df = pd.read_parquet(\"export/ML_algo_reg_res/results_reg_df.gzip\")\n",
    "best_models = pd.read_parquet(\"export/ML_algo_reg_res/best_models.gzip\")\n",
    "best_models1 = pd.read_parquet(\"export/ML_algo_reg_res/best_models1.gzip\") \n",
    "to_improve = pd.read_parquet(\"export/ML_algo_reg_res/to_improve.gzip\")\n",
    "results_grid2 = pd.read_parquet(\"export/ML_algo_reg_res/results_grid2.gzip\")\n",
    "results_grid = results_grid2.to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca61e8e-20fc-49a3-9bec-6dd99126c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f71cb-9b16-4eaa-ade6-1fa505c126dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cba33a-9a39-4a34-b020-4310e916f032",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 4.1.3. Algorithmic Regression Models Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116de341-ba06-4dcb-8b4c-c7c3ef4ecd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_reg_analysis = results_reg_df.astype(float)\n",
    "res_reg_analysis.columns = res_reg_analysis.columns.map('_'.join)\n",
    "res_reg_analysis = res_reg_analysis.reset_index()\n",
    "res_reg_analysis[\"coef\"] = res_reg_analysis.datain.str.extract('([0-9][.]*[0-9]*)')\n",
    "res_reg_analysis[\"scaling\"] = res_reg_analysis.datain.str.split(\"_\", expand = True)[2]\n",
    "res_reg_analysis = res_reg_analysis.iloc[:,[0,1,-2,-1,3,4,6,7,8,9,10,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbde2f-685c-430a-9dfb-44b3ea3b8f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_reg_analysis_grp = pd.DataFrame(res_reg_analysis.groupby([\"ticker\",\"model\",\"coef\",\"n_features\",\"scaling\",\"test_size_prop\"])[\"train_score\",\"test_score\"].mean())\n",
    "res_reg_analysis_grp = res_reg_analysis_grp.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae3032-d88e-430c-8c41-7ad1cd9e4465",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_reg_analysis_grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35b5e5-13ab-4db1-9e00-2040cc042184",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(res_reg_analysis_grp, x = \"train_score\", y = \"test_score\", marginal_x = \"histogram\", marginal_y = \"box\", \n",
    "           color = \"model\", size = \"n_features\", trendline = \"ols\",\n",
    "           range_x = [0.6,1], range_y = [0.6,1],\n",
    "           height = 600, width=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9809f5-ee27-4aa8-9a2b-b10d6cfef69e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b7b60-f01c-4a75-a53f-28d883c62505",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1.4. Best Models Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31428a-5879-4dfa-96f7-4e7d288f2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dict()\n",
    "y = dict()\n",
    "sc = dict()\n",
    "X_train = dict()\n",
    "X_test = dict()\n",
    "y_train = dict()\n",
    "y_test = dict()\n",
    "estimator = dict()\n",
    "estimator_fit = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4eae82-6f00-4f75-bfe4-dbe7f9df361f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## RETURNS SC AND ESTIMATOR (AS A NON TRAINED PIPELINE) PER TICKER\n",
    "now1 = datetime.now()\n",
    "## HEADER\n",
    "\n",
    "## MODEL SELECTION\n",
    "matcorr = matcorr_data2y_ret\n",
    "res_df = best_models\n",
    "# Shifting steps\n",
    "S = 10\n",
    "\n",
    "for n in range(res_df.index.size):\n",
    "    target_tic = res_df.index[n][0]\n",
    "    model = res_df.index[n][1]\n",
    "    datain = res_df.index[n][2]\n",
    "    test_size_prop = float(res_df.index[n][4])\n",
    "    target_tic_stock = (target_tic,\"Close\")\n",
    "    flag_nosc = True\n",
    "    \n",
    "    ## DATA\n",
    "    if datain.split(\"_\")[0] == \"rets\":\n",
    "        df = data2y_ret\n",
    "    else:\n",
    "        print(\"There is a problem with initial df\")\n",
    "        \n",
    "    ## MODEL SELECTION\n",
    "    if datain.split(\"_\")[1].find(\"corr\") != -1: # CORRELATION SELECTION\n",
    "        coef = float(datain.split(\"_\")[1][datain.split(\"_\")[1].find(\"0\"):])\n",
    "        df_corr_coef = matcorr[target_tic][(matcorr[target_tic] > coef) | (matcorr[target_tic] < -coef)][\"Close\"].dropna(how=\"all\")\n",
    "        ticks_corr = df_corr_coef.index.unique()\n",
    "        ticks_corr = np.delete(ticks_corr,np.where(ticks_corr == target_tic_stock))\n",
    "        # X[target_tic] = df[ticks_corr].values\n",
    "        # y[target_tic] = df[target_tic_stock].values\n",
    "        X[target_tic] = df[ticks_corr].iloc[:-S].dropna().values  # Correlated Features truncated S steps in the end\n",
    "        y[target_tic] = df[target_tic_stock].shift(S).dropna().values # Target shifted S steps\n",
    "\n",
    "    elif datain.split(\"_\")[1].find(\"kbest\") != -1: # KBEST SELECTION\n",
    "        n = int(datain.split(\"_\")[1][datain.split(\"_\")[1].find(\"t\")+1:])\n",
    "        # X[target_tic] = df.iloc[:,np.where(df.columns != target_tic_stock)[0]]\n",
    "        # y[target_tic] = df[target_tic_stock].values\n",
    "        X[target_tic] = df.iloc[:,np.where(df.columns != target_tic_stock)[0]].iloc[:-S].dropna()  # Correlated Features truncated S steps in the end\n",
    "        y[target_tic] = df[target_tic_stock].shift(S).dropna().values # Target shifted S steps\n",
    "        \n",
    "        bestfeatures = SelectKBest(score_func=f_regression, k=n)\n",
    "        fit = bestfeatures.fit(X[target_tic],y[target_tic])\n",
    "        dfscores = pd.DataFrame(fit.scores_)\n",
    "        dfcolumns = pd.DataFrame(X[target_tic].columns)\n",
    "        featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "        featureScores.columns = ['Specs','Score']\n",
    "        # X[target_tic] = df[featureScores.nlargest(n,'Score').Specs.values].values\n",
    "        X[target_tic] = df[featureScores.nlargest(n,'Score').Specs.values].iloc[:-S].dropna().values\n",
    "        \n",
    "    elif datain.split(\"_\")[1].find(\"_pca\") != -1: # PCA SELECTION\n",
    "        pass\n",
    "    else:\n",
    "        print(\"There is a problem with model selection\")\n",
    "    \n",
    "    ## DATA SCALING \n",
    "    for i in range(len(scalers)):\n",
    "        if str(scalers[i]).find(datain.split(\"_\")[2][2:]) == 0:\n",
    "            flag_nosc = False\n",
    "            # X[target_tic] = scalers[i].fit_transform(X[target_tic])  \n",
    "            # print(f\"{target_tic} scaled OK\")\n",
    "            sc[target_tic] = scalers[i]\n",
    "    if flag_nosc:\n",
    "        # X[target_tic] = np.array(X[target_tic])\n",
    "        sc[target_tic] = \"nosc\"\n",
    "        print(f\"{target_tic} not scaled\")\n",
    "    \n",
    "    ## HYPER-PARAMETERS OPTIMIZATION. GRID SEARCH\n",
    "    if model.split(\"_\")[1] == \"reg\":\n",
    "        try:\n",
    "            ## SPLITS DEFINITION\n",
    "            best_ind = X[target_tic].shape[0]-int(X[target_tic].shape[0]*test_size_prop)\n",
    "            X_train[target_tic], X_test[target_tic] = X[target_tic][:best_ind,:], X[target_tic][best_ind:,:]\n",
    "            y_train[target_tic], y_test[target_tic] = y[target_tic][:best_ind], y[target_tic][best_ind:]\n",
    "            \n",
    "            ## BEST MODEL INSTANTIATION\n",
    "            estimator[target_tic] = reg_estimator(model, target_tic,\n",
    "                                                  X_train[target_tic], y_train[target_tic],\n",
    "                                                  datain, results_grid, test_size_prop, sc[target_tic])\n",
    "        except:\n",
    "            print(f\"There is a problem modelling {target_tic} \")\n",
    "    elif model.split(\"_\")[1] == \"clf\":\n",
    "        try:\n",
    "            ## SPLITS DEFINITION\n",
    "            best_ind = X[target_tic].shape[0]-int(X[target_tic].shape[0]*test_size_prop)\n",
    "            X_train[target_tic], X_test[target_tic] = X[target_tic][:best_ind,:], X[target_tic][best_ind:,:]\n",
    "            y_train[target_tic], y_test[target_tic] = y[target_tic][:best_ind], y[target_tic][best_ind:]\n",
    "            \n",
    "            ## BEST MODEL INSTANTIATION\n",
    "            estimator[target_tic] = clf_estimator(model, target_tic,\n",
    "                                                  X_train[target_tic], y_train[target_tic], \n",
    "                                                  datain, results_grid, test_size_prop, sc[target_tic])\n",
    "        except:\n",
    "            print(f\"There is a problem modelling {target_tic} \")\n",
    "\n",
    "now2 = datetime.now()\n",
    "delta = now2 - now1\n",
    "print(f\"ML Optimization Done! - Process time: {delta.seconds} seconds ({np.round(delta.seconds / 60,2)} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def8a94-41ce-4eed-a1af-1e951b5d391c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34324c7-c36e-4f18-86fd-19a761a434e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6906c3f4-19c0-466f-9af1-5237b34cd511",
   "metadata": {
    "tags": []
   },
   "source": [
    "ohlcv_cols = data.columns.get_level_values(1).unique()[:-2].values\n",
    "\n",
    "### LAST 2 YEARS RETURNS\n",
    "data2y_ret = data.pct_change().dropna(how=\"all\").fillna(0).loc[\"2020\":,(ticks,ohlcv_cols)]\n",
    "data2y_ret.replace([np.inf,-np.inf],0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc7c24-903b-418b-9f86-3090dea7baad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reg_pred_chart(target_tic):\n",
    "    ind = X_test[target_tic].shape[0]\n",
    "    close = data[target_tic][\"Close\"][-ind:]\n",
    "    close_ret = data2y_ret[target_tic][\"Close\"][-ind:]\n",
    "    close_ret_cum = close_ret.cumsum()\n",
    "    \n",
    "    fc = pd.Series(estimator[target_tic].fit(X_train[target_tic], y_train[target_tic]).predict(X_test[target_tic]).cumsum(), index = close.index)\n",
    "    \n",
    "    df_xpred = pd.concat([close, close_ret, close_ret_cum, fc], axis=1)\n",
    "    df_xpred.columns = [\"close\",\"rets\",\"cumrets\",\"cumrets_pred\"]\n",
    "    \n",
    "    plt.figure(figsize = (20,8))\n",
    "    plt.suptitle(f\"Ticker: {target_tic} | Last Cum Return: {df_xpred.iloc[-1,2].round(2)}| Price Return: {((df_xpred.iloc[-1,0] - df_xpred.iloc[0,0]) / df_xpred.iloc[0,0]).round(2)} | Score: {np.round(r2_score(y_test[target_tic], estimator[target_tic].fit(X_train[target_tic], y_train[target_tic]).predict(X_test[target_tic])),2)}\",\n",
    "                fontsize = 25)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.plot(data[target_tic][\"Close\"][-250:-ind+1], \"--r\", label = f\"{df_xpred.iloc[:,0].name}\")\n",
    "    ax.plot(df_xpred.iloc[:,0], \"r\", label = f\"{df_xpred.iloc[:,0].name}\")\n",
    "    ax.legend(loc=\"upper left\", fontsize = 15)\n",
    "    \n",
    "    ax2 = plt.twinx()\n",
    "    ax2.plot(df_xpred.iloc[:,2], \"k\",label = f\"{df_xpred.iloc[:,2].name}\")\n",
    "    ax2.plot(df_xpred.iloc[:,3], \"ok\", label = f\"{df_xpred.iloc[:,3].name}\")\n",
    "    ax.grid(False)\n",
    "    ax2.legend(loc=\"lower left\", fontsize = 15)\n",
    "    ax2.set_ylim(-0.3,0.3)\n",
    "    plt.show()\n",
    "    \n",
    "interact(reg_pred_chart, target_tic = widgets.Dropdown(options=best_models.index.get_level_values(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5233d67-9621-40f4-9282-6ff3052f122e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_tic_stock = (\"AAPL\",\"Close\")\n",
    "S = 5\n",
    "fig, ax = plt.subplots(2,1,figsize=(25,8))\n",
    "ax1 = ax[0]\n",
    "ax2 = ax[1]\n",
    "start6 = -100\n",
    "\n",
    "xx = df[ticks_corr]\n",
    "xxs = df[ticks_corr].iloc[:-S].dropna()\n",
    "xxf = df[ticks_corr].iloc[S:]\n",
    "\n",
    "yy = df[target_tic_stock]\n",
    "yys = df[target_tic_stock].shift(S).dropna()\n",
    "\n",
    "ax1.plot(xx.iloc[start6:,[40]], c = \"orange\", linewidth = 3, alpha = 0.8, label = \"X\")\n",
    "ax1.plot(xx.iloc[start6:,], c = \"gray\", linewidth = 3, alpha = 0.01)\n",
    "ax1.axvline(xx.iloc[-S-1].name, ls = \"--\", c=\"gray\")\n",
    "\n",
    "ax1.plot(xxs.iloc[start6:,[40]], c = \"red\", linewidth = 3, alpha = 0.8, label = f\"X cut {S}\")\n",
    "ax1.plot(xxs.iloc[start6:,], c = \"gray\", linewidth = 3, alpha = 0.01)\n",
    "\n",
    "ax1.plot(xxf.iloc[start6:,[40]], \"o\")\n",
    "\n",
    "ax1.set_ylabel(\"X\", fontsize = 20)\n",
    "ax1.legend(loc = \"upper left\", fontsize = 15)\n",
    "ax1.set_ylim(-.06,.05)\n",
    "\n",
    "ax2.plot(yy.iloc[start6:], c = \"lightblue\", linewidth = 2, label = \"y\")\n",
    "ax2.plot(yys.iloc[start6:], c = \"blue\", linewidth = 3, label = f\"y shifted {S}\")\n",
    "ax2.axvline(xx.iloc[-S-1].name, ls = \"--\", c=\"gray\")\n",
    "\n",
    "ax2.set_ylabel(\"y\", fontsize = 20)\n",
    "ax2.legend(loc = \"upper left\", fontsize = 15)\n",
    "ax2.set_ylim(-.05,.05)\n",
    "\n",
    "print(f\"X shape: {xx.shape} | y shape: {yy.shape} | X shifted shape: {xxs.shape} | y shifted shape: {yys.shape} \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9421d-0551-4438-85c6-31d2580b0011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = dict()\n",
    "y = dict()\n",
    "X_future = dict()\n",
    "X_train = dict()\n",
    "X_test = dict()\n",
    "\n",
    "def reg_pred_chart(target_tic, S, model):\n",
    "    header = True\n",
    "    datadef = True\n",
    "    splits = True\n",
    "    pred = True\n",
    "    chart = True    \n",
    "    if header:\n",
    "        matcorr = matcorr_data2y_ret\n",
    "        res_df = best_models\n",
    "        \n",
    "        datain = res_df.loc[res_df.index.get_level_values(0) == target_tic,:].index[0][2]\n",
    "        test_size_prop = float(res_df.loc[res_df.index.get_level_values(0) == target_tic,:].index[0][4])\n",
    "        target_tic_stock = (target_tic,\"Close\")\n",
    "        df = data2y_ret.copy()\n",
    "        \n",
    "        coef = float(datain.split(\"_\")[1][datain.split(\"_\")[1].find(\"0\"):])\n",
    "        df_corr_coef = matcorr[target_tic][(matcorr[target_tic] > coef) | (matcorr[target_tic] < -coef)][\"Close\"].dropna(how=\"all\")\n",
    "        ticks_corr = df_corr_coef.index.unique()\n",
    "        ticks_corr = np.delete(ticks_corr,np.where(ticks_corr == target_tic_stock))\n",
    "\n",
    "    if datadef:\n",
    "        X[target_tic] = df[ticks_corr].iloc[:-S].values  # Correlated Features truncated S steps in the end\n",
    "        y = df[target_tic_stock].shift(S).dropna()\n",
    "        y[target_tic] = y.values # Target shifted S steps\n",
    "        X_future[target_tic] = df[ticks_corr].iloc[-S:] # Correlated Features in last S steps. USED TO PREDICT TARGET'S FUTURE\n",
    "\n",
    "    if splits:\n",
    "        ## SPLITS DEFINITION\n",
    "        best_testind = X[target_tic].shape[0]-int(X[target_tic].shape[0]*test_size_prop)\n",
    "        X_train[target_tic], X_test[target_tic] = X[target_tic][:best_testind,:], X[target_tic][best_testind:,:]\n",
    "        y_train[target_tic], y_test[target_tic] = y[target_tic][:best_testind], y[target_tic][best_testind:]\n",
    "    \n",
    "        testind = X_test[target_tic].shape[0]\n",
    "        close_bef_test = data[target_tic][\"Close\"][-120:-testind-S+1] # Close Prices before test period\n",
    "        close_test = data[target_tic][\"Close\"][-testind-S:] # Close Prices in test period\n",
    "        \n",
    "        close_ret_test = data2y_ret[target_tic][\"Close\"][-testind-S:-S] # Close Returns in test period\n",
    "        close_ret_test_cum = close_ret_test.cumsum() # Close Returns Cum in test period\n",
    "        \n",
    "        close_ret_fut = data2y_ret[target_tic][\"Close\"][-S-1:] # Close Returns in future period\n",
    "        close_ret_fut_cum = close_ret_fut.cumsum() # Close Returns Cum in test period\n",
    "        \n",
    "    if pred:\n",
    "        est = reg_estimator(model, target_tic, X[target_tic], y[target_tic], datain, results_grid, test_size_prop, sc[target_tic])\\\n",
    "        .fit(X_train[target_tic], y_train[target_tic])\n",
    "        fc_test = est.predict(X_test[target_tic])\n",
    "        fc_test = pd.Series(fc_test, index = df[ticks_corr].iloc[best_testind:-S].index)\n",
    "        fc_test_cum = pd.Series(fc_test.cumsum(), index = df[ticks_corr].iloc[best_testind:-S].index)\n",
    "    \n",
    "        fc_future = est.predict(X_future[target_tic])\n",
    "        fc_future = pd.Series(fc_future, index = df[ticks_corr].iloc[-S:].index)\n",
    "        fc_future_cum = pd.Series(fc_future.cumsum(), index = df[ticks_corr].iloc[-S:].index)\n",
    "\n",
    "    if chart:\n",
    "        fig, ax = plt.subplots(2,1,figsize = (20,10))\n",
    "        plt.suptitle(f\"Ticker: {target_tic} | Score: {np.round(r2_score(y_test[target_tic], fc_test),2)} | Last Cum Return: {close_ret_test_cum.iloc[-S].round(2)} | Last Pred Test Cum Return: {fc_test_cum.iloc[-1].round(2)} | Price Return: {((close_test.iloc[-1] - close_test.iloc[0]) / close_test.iloc[0]).round(2)}\", fontsize = 22)\n",
    "        \n",
    "        ax1 = ax[0]\n",
    "        ax2 = ax[1]\n",
    "        ax1.grid(False)\n",
    "        ax1.plot(close_bef_test, \"--r\", label = \"close before test\")\n",
    "        ax1.plot(close_test, \"r\", label = \"close test steps\")  \n",
    "        ax1.legend(loc=\"upper left\", fontsize = 15)\n",
    "        ax1.set_ylabel(\"Close prices\", fontsize = 18)\n",
    "        ax1.get_xaxis().set_visible(False)\n",
    "        \n",
    "        ax12 = ax1.twinx()  \n",
    "        ax12.plot(data2y_ret[ticks_corr][-S:].cumsum() + data2y_ret[ticks_corr][-testind-S:-S].cumsum().iloc[-1], \"gray\", alpha = 0.05) # MUST SUM TEST LAST CUMSUM VALUE TO PREDICTION TO AVOID JUMP BETWEEN BOTH, IN THE NEXT TWO PREDICTIONS\n",
    "        ax12.plot(data2y_ret[ticks_corr][-testind-S:-S+1].cumsum(), \"gray\", alpha = 0.05)\n",
    "        # ax12.plot(y.cumsum(), \"k\", alpha = 0.5, label = \"target shifted\")\n",
    "        \n",
    "        ax12.plot(close_ret_test_cum, \"k\",label = \"cumrets\")\n",
    "        ax12.plot(fc_test_cum, \"ok\", label = \"cumrets_test_pred\")\n",
    "        \n",
    "        ax12.plot(close_ret_fut_cum + close_ret_test_cum[-2], \"g\", label = \"cumrets_fut_hist\") # MUST SUM TEST LAST CUMSUM VALUE TO PREDICTION TO AVOID JUMP BETWEEN BOTH, IN THE NEXT TWO PREDICTIONS   \n",
    "        ax12.plot(fc_future_cum + fc_test_cum[-1], \"og\", label = \"cumrets_fut_pred\")    \n",
    "        ax12.legend(loc=\"lower left\", fontsize = 15)\n",
    "        ax12.set_ylim(np.min(close_ret_test_cum),np.max(close_ret_test_cum))\n",
    "        \n",
    "        ax2.plot(data2y_ret[ticks_corr][-testind-S:-S+1], \"gray\", alpha = 0.05)\n",
    "        ax2.plot(close_ret_test, \"k\", label = \"rets\")\n",
    "        \n",
    "        ax2.plot(fc_test, \"ok\", label = \"rets_test_pred\")\n",
    "        ax2.plot(data2y_ret[ticks_corr][-S:], \"gray\", alpha = 0.05)\n",
    "        \n",
    "        ax2.plot(close_ret_fut, \"g\", label = \"rets_fut_hist\")\n",
    "        ax2.plot(fc_future, \"og\", label = \"rets_fut_pred\")\n",
    "        ax2.set_ylabel(\"Cum rets\", fontsize = 18)\n",
    "        ax2.legend(loc=\"lower left\", fontsize = 15)\n",
    "        ax2.set_ylim(np.min(close_ret_test),np.max(close_ret_test))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "interact(reg_pred_chart, target_tic = widgets.Dropdown(options=best_models.index.get_level_values(0)), \n",
    "         S = widgets.IntSlider(min=1, max = 20, value = 5)  ,\n",
    "         model = widgets.Dropdown(options = ['lin_reg', 'ridge_reg', 'elastic_reg','bayridge_reg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f26223-9c86-472f-969b-3fa19fa926c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[target_tic] = data2y_ret[ticks_corr].iloc[:-50,15].values  #\n",
    "y[target_tic] = data2y_ret[target_tic_stock].shift(50).dropna().values # Target shifted S steps\n",
    "\n",
    "px.scatter(x = X[target_tic], y = y[target_tic], range_x = [-0.2,0.2], range_y = [-0.2,0.2], trendline = \"ols\", \n",
    "           title = f\"{np.corrcoef(X[target_tic], y[target_tic])[0][1].round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775adf39-e6fa-4e48-a9c9-7e90bca61384",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[target_tic] = data2y_ret[ticks_corr].iloc[:,15].values  #\n",
    "y[target_tic] = data2y_ret[target_tic_stock].values # Target shifted S steps\n",
    "px.scatter(x = X[target_tic], y = y[target_tic], range_x = [-0.2,0.2], range_y = [-0.2,0.2], trendline = \"ols\", \n",
    "           title = f\"{np.corrcoef(X[target_tic], y[target_tic])[0][1].round(2)}\", )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbc240c7-b62a-4aaf-9475-51ea474d0100",
   "metadata": {},
   "source": [
    "def unfold_ts_for_regression(data,look_back=20,look_ahead=1,):\n",
    "    \"\"\"Unfolds ts for regression.\n",
    "     \n",
    "    This functions receives as input a time series and returns two sets, X and\n",
    "    y.\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame, pandas.Series or numpy.array\n",
    "        The time series to process.\n",
    "    look_back : int, optional, default: 20\n",
    "        The number of days to look back to predict the next day.\n",
    "    look_ahead : int, optional, default: 0\n",
    "        If 'look_ahead' is 1, the label will be the next data of the \n",
    "        batch. If it is greater, the labels will be 'look_ahead' data of the\n",
    "        batch.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy.array\n",
    "        An array containing the features.\n",
    "    y : numpy.array\n",
    "        An array containing the labels.\n",
    "     \n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame) or isinstance(data, pd.Series):\n",
    "        data = data.values\n",
    " \n",
    "    elif isinstance(data, list):\n",
    "        data = np.array(data)\n",
    "         \n",
    "    elif isinstance(data, np.ndarray):\n",
    "        pass\n",
    "         \n",
    "    else:\n",
    "        raise TypeError(f\"Non-supported data type: {type(data)}\")\n",
    "     \n",
    "    X = []\n",
    "    y = []\n",
    "     \n",
    "    if look_ahead == 1:\n",
    "        _range = range(0, len(data) - look_back)\n",
    "    else:\n",
    "        _range = range(0, len(data) - look_back - look_ahead)\n",
    "     \n",
    "    for idx in _range:\n",
    "        batch_end = idx + look_back\n",
    "        ahead_end = batch_end + look_ahead - 1\n",
    " \n",
    "        local_X = data[idx:batch_end]\n",
    "        local_y = data[ahead_end]\n",
    " \n",
    "        X.append(local_X)\n",
    "        y.append(local_y)\n",
    "     \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b6c7e-46fb-491c-9bb8-e7f4552305b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[\"AAPL\"].Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdeb205-0786-4141-a175-7cf948a7390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 20\n",
    "look_ahead = 10\n",
    "\n",
    "df = data[\"AAPL\"].Close\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "if look_ahead == 1:\n",
    "    _range = range(0, len(df) - look_back)\n",
    "else:\n",
    "    _range = range(0, len(df) - look_back - look_ahead)\n",
    " \n",
    "for idx in _range:\n",
    "    batch_end = idx + look_back\n",
    "    ahead_end = batch_end + look_ahead - 1\n",
    "    # print(batch_end, ahead_end)\n",
    "    local_X = df[idx:batch_end]\n",
    "    local_y = df[ahead_end]\n",
    "    a.append(local_X)\n",
    "    b.append(local_y)\n",
    "a = np.array(a)\n",
    "b = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf71617-affd-4227-bc2a-40b47826325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800e1a3-c9c6-4d63-b218-9a08e01a2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(df[-180:])\n",
    "ax2 = ax.twiny()\n",
    "ax2.plot(a[-180:])\n",
    "ax2.plot(b[-180:], linewidth = 3, c = \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19cd9d64-4715-47c0-8784-c57c3260671b",
   "metadata": {},
   "source": [
    "max_size = 0\n",
    "for k in results_cv.keys():\n",
    "    if int(list(k)[3].split(\"_\")[1]) > max_size:\n",
    "        max_size = int(list(k)[3].split(\"_\")[1])\n",
    "        \n",
    "results_reg_df = pd.DataFrame(index = results_cv.keys(), columns = [f\"split_{i}\" for i in range(max_size)])\n",
    "for k, v in results_cv.items():\n",
    "    train_r2.loc[k] = np.round(results_cv[k][\"train_r2\"],2)\n",
    "    \n",
    "train_r2[(\"train_mean\")] = np.round(train_r2.mean(1),2)\n",
    "train_r2[(\"train_std\")] = np.round(train_r2.std(1),3)\n",
    "\n",
    "test_r2 = pd.DataFrame(index = results_cv.keys(), columns = [f\"split_{i}\" for i in range(max_size)])\n",
    "for k, v in results_cv.items():\n",
    "    test_r2.loc[k] = np.round(results_cv[k][\"test_r2\"],2)\n",
    "test_r2[(\"test_mean\")] = np.round(test_r2.mean(1),2)\n",
    "test_r2[(\"test_std\")] = np.round(test_r2.std(1),3)   \n",
    "\n",
    "train_test = pd.concat([train_r2.iloc[:,-2:],test_r2.iloc[:,-2:]], axis = 1)\n",
    "# train_test[\"diff_mean\"] = train_test[\"train_mean\"]-train_test[\"test_mean\"]\n",
    "# train_test[\"diff_std\"] = train_test[\"train_std\"]-train_test[\"test_std\"]\n",
    "train_test.index.names = [\"ticker\",\"model\",\"datain\"]\n",
    "best_models_prel = train_test.loc[train_test.groupby([\"ticker\"])[\"test_mean\"].idxmax().values,:]\n",
    "best_models_prel\n",
    "\n",
    "to_improve = best_models_prel[best_models_prel[\"test_mean\"] < 0.86]\n",
    "best_models = best_models_prel[best_models_prel.index.isin(to_improve.index) == False]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d409dacb-0740-4653-b106-94ac65c15834",
   "metadata": {},
   "source": [
    "ensembles\n",
    "RandomForestRegressor, AdaBoostRegressor, VotingRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "215d5ca4-216f-417e-830f-ec4de1cce99f",
   "metadata": {},
   "source": [
    "train_test.groupby(\"ticker\").agg(test_mean=('test_mean',lambda x:x[train_test.loc[x.index,'test_mean'].idxmax()]),\n",
    "                     test_std = ('test_std',lambda x:x[train_test.loc[x.index,'test_std'].idxmin()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2fe06-c071-4c08-a169-34b686d14e9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.1.5. Individual Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc495783-9982-4bde-a3e3-8e6bed00b956",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 4.2. Model Selection"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a87e047-fb85-486f-97f0-39c784b27553",
   "metadata": {},
   "source": [
    "data2y_ret.columns.map(lambda idx: f'{idx[0]}-{idx[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546333a7-b76f-4c18-bfc2-c664e226b78b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.2.1. Correlation Matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a617a39c-bd63-4eb0-b724-545caa556119",
   "metadata": {
    "tags": []
   },
   "source": [
    "## returns DF\n",
    "target_tic = ticks_strategy[0]\n",
    "target_tic_stock = (target_tic,\"Close\")\n",
    "\n",
    "coef_r = 0.6\n",
    "matcorr = matcorr_data2y_ret\n",
    "df_corr_coef_r = matcorr[target_tic][(matcorr[target_tic] > coef_r) | (matcorr[target_tic] < -coef_r)][\"Close\"].dropna(how=\"all\")\n",
    "ticks_corr_r = df_corr_coef_r.index.unique()\n",
    "ticks_corr_r = np.delete(ticks_corr_r,np.where(ticks_corr_r == target_tic_stock))\n",
    "\n",
    "Xr = data2y_ret[ticks_corr_r]\n",
    "yr = data2y_ret[target_tic_stock]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49889674-ee1c-4389-b9ae-ad0e8a6423e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.2.2. SelectKBest"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34967ed5-b146-47c8-ba54-e718822ca253",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data without correlation factor filter (Complete Returns Dataset)\n",
    "Xr2 = data2y_ret.iloc[:,np.where(data2y_ret.columns != (\"AAPL\",\"Close\"))[0]]\n",
    "yr2 = data2y_ret[target_tic_stock]\n",
    "\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=100)\n",
    "fit = bestfeatures.fit(Xr2,yr2)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(Xr2.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "\n",
    "sns.distplot(featureScores.Score[featureScores.Score>0], label = \"Complete DF\")\n",
    "sns.distplot(featureScores.nlargest(100,'Score').Score, label = \"Best Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cee31d-19a0-4018-a618-2630aacd0146",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3. Data Input & Transformations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29dc6c01-8b9a-494c-9c5f-2b1b3b2dd81d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#SCALE\n",
    "sc = StandardScaler()\n",
    "Xr_sc = sc.fit_transform(Xr)\n",
    "# Xcr_sc = sc.fit_transform(Xcr)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "443ada52-67ea-48e1-be59-0307fdc38970",
   "metadata": {},
   "source": [
    "# General variables\n",
    "results_cv_ind = dict()\n",
    "results_grid_ind = dict()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56bdbe26-14cf-461b-ab49-1a085a4c8cd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "ax1 = plt.subplot(1,2,1)\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax1.boxplot(X_sc)\n",
    "ax2.boxplot(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b49cc767-60db-47cf-9c0e-e5e7642e7269",
   "metadata": {
    "tags": []
   },
   "source": [
    "i = 2\n",
    "\n",
    "plt.suptitle(\"Difference between original and scaled data\", fontsize = 20)\n",
    "ax = plt.gca()\n",
    "ax.plot(Xr.iloc[:,i],\"o\", label = \"X_train\")\n",
    "ax.legend(loc = (0.001,0.92))\n",
    "\n",
    "ax2 = plt.twiny()\n",
    "ax2.plot(Xr_sc[i],\"ro\",alpha = 0.3, label = \"X_train_sc\")\n",
    "\n",
    "ax2.set_axis_off()\n",
    "ax2.legend(loc = (0.001,0.86))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d81613-4900-4557-b88b-d89a363f361d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.1. Linear Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "452f5d5b-3641-4d4b-90c1-ad3f1386388d",
   "metadata": {},
   "source": [
    "model = \"lin_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = LinearRegression()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a40b8b9f-cef0-4b26-918d-0cdbf8c3c338",
   "metadata": {},
   "source": [
    "def test_size_opt(estimator, X, y, target_tic, model, datain):\n",
    "    global ts_opt_res\n",
    "    global results_cv\n",
    "    test_size_porc = [np.round(0.05*x,2) for x in range(1,8)]\n",
    "    \n",
    "    for ts in test_size_porc:\n",
    "        n_splits = int(np.trunc(X.shape[0]/int(X.shape[0]*ts)))\n",
    "        test_size = int(X.shape[0]*ts)\n",
    "        tscv = TimeSeriesSplit(n_splits = n_splits, gap = 0, test_size = test_size-1)\n",
    "        \n",
    "        results_cv[(target_tic,model,datain,ts)] = cross_validate(estimator, \n",
    "                                                                   X, y, \n",
    "                                                                   cv = tscv, \n",
    "                                                                   scoring = scoring, \n",
    "                                                                   return_train_score = True)\n",
    "        ind_best_res_ts = np.argmax(results_cv[(target_tic,model,datain,ts)][\"test_r2\"])\n",
    "        ts_opt_res[target_tic,model,datain,ts, ind_best_res_ts] = np.round(np.max(results_cv[(target_tic,model,datain,ts)][\"test_r2\"]),2)\n",
    "        \n",
    "    ##BEST RESULTS\n",
    "    best_test_size_porc = max(ts_opt_res, key = ts_opt_res.get)[3] # BEST TEST SIZE PROPORTION\n",
    "    n_splits = int(np.floor(X.shape[0]/int(X.shape[0]*best_test_size_porc)))\n",
    "    test_size = int(X.shape[0]*best_test_size_porc)\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, gap = 0, test_size = test_size)\n",
    "    \n",
    "    train_inds = np.zeros(0)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        test_ind = test_index.size\n",
    "        train_inds = np.append(train_inds,train_index.size)\n",
    "    \n",
    "    best_ind = int(train_inds[max(ts_opt_res, key = ts_opt_res.get)[4]]) # BEST TEST SIZE PROPORTION\n",
    "    \n",
    "    # SPLITS DEFINITION\n",
    "    X_train, X_test = X[:best_ind,:], X[best_ind:,:]\n",
    "    y_train, y_test = y[:best_ind], y[best_ind:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4457260-ffdf-459f-bdbe-b5ff9cb21b1a",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = test_size_opt(estimator, Xr_sc, yr,target_tic,model,datain)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "622edb52-091c-4a9a-9089-4c0a38e663b7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "results_cv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45b77dd4-f348-45c7-99c6-37d34a09786e",
   "metadata": {},
   "source": [
    "ts_opt_res"
   ]
  },
  {
   "cell_type": "raw",
   "id": "242006df-2bae-4c41-a29f-54aaecc632a5",
   "metadata": {},
   "source": [
    "fig,ax = plt.subplots(1,2, figsize =(25,7))\n",
    "ax[0].plot(X_train[:,1],c=\"k\", label = \"X train\")\n",
    "ax[0].plot(pd.DataFrame(X_test[:,1],index = range(X_train.shape[0], X_train.shape[0]+X_test.shape[0])), c=\"orange\", label = \"X test\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(y_train, c=\"k\", label = \"target train\")\n",
    "ax[1].plot(y_test, c=\"orange\", label = \"target test\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d88af80f-fa46-46b3-947a-cbbe17d62151",
   "metadata": {
    "tags": []
   },
   "source": [
    "scoring = [\"r2\"]\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = tscv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] ,\n",
    "                     ylim1 = 0, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b52e00b2-82f4-4522-acc6-ab366f4d48c5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d529c-fe5e-460b-bac4-d1fe89f2a4fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.2. Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c54e7b-f472-44dd-b46c-e37b4dc32d62",
   "metadata": {},
   "source": [
    "**alpha** is de regularization parameter, that makes you control complexity. Higher values of it, forces coefficients to move towards zero \n",
    "and increases the restriction on the model; decreases training peformance but also increases the generability of the model. -> Underfit.\n",
    "Overfit is biceversa; with lower values of alpha, that trends to linear regression."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce5410f0-0dbb-4b6a-8f1b-86a1fcf39aee",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = \"ridge_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = Ridge()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1578b046-ed81-41b3-90bc-10e1f5b36900",
   "metadata": {
    "tags": []
   },
   "source": [
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha=alpha)\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b134419-bff7-4e4b-9350-71ccf63ecc50",
   "metadata": {
    "tags": []
   },
   "source": [
    "# grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb3dbb5e-e08a-495f-b9fa-70bae01cd7d1",
   "metadata": {},
   "source": [
    "estimator = Ridge(alpha = results_grid_ind[(target_tic,model,datain)][\"alpha\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f518924-7973-46cc-9537-fd0986173f7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "scoring = [\"r2\"]\n",
    "cv = ShuffleSplit(n_splits=10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a68bf116-e0b8-4bda-8803-988c24a25387",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9cebf7-cfe2-4b22-b984-b5cfc4a27a16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.3. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da04e62-c63c-4592-bacd-452e6c9ae18a",
   "metadata": {},
   "source": [
    "**alpha** is de regularization parameter, that makes you control complexity. Higher values of it, forces coefficients to move towards zero \n",
    "and increases the restriction on the model; decreases training peformance but also increases the generability of the model. -> Underfit.\n",
    "Overfit is biceversa; with lower values of alpha, that trends to linear regression."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bff1f3c5-c9f6-4918-9406-c146aa38a0bf",
   "metadata": {},
   "source": [
    "model = \"lasso_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = Lasso()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e08dfae-8ddb-423b-92f2-49fa02a8c936",
   "metadata": {},
   "source": [
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha = alpha)\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a321509-12aa-49c6-a29c-0a62b247d6ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "578c5cf7-01a6-43b6-aebe-87feaf502cfc",
   "metadata": {},
   "source": [
    "estimator = Lasso(alpha = results_grid_ind[(target_tic,model,datain)][\"alpha\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff5b9b8d-dfd4-4d2d-b299-c1d0ef05d15b",
   "metadata": {
    "tags": []
   },
   "source": [
    "scoring = [\"r2\"]\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = tscv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] ,\n",
    "                     ylim1 = 0, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7b34c5f-2fcc-4d94-850a-32462f4c0a4a",
   "metadata": {},
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "365b8552-4912-4077-9adc-0c7e2ae9e1ff",
   "metadata": {},
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01808743-0481-497f-a876-117d42195722",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.4. Elastic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255da742-c4b6-4b3d-b078-cb2f655db80a",
   "metadata": {},
   "source": [
    "**l1 parameter** is the combination factor between lasso and ridge. If l1 = 0, we have Ridge, if l1 = 1, we have Lasso. Between 0 and 1 a convination of both"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b537f4e3-7a30-4e05-9d82-f6dd522db83e",
   "metadata": {},
   "source": [
    "model = \"elastic_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = ElasticNet()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "468da521-d7c3-4ffc-a6f6-776d52159bb3",
   "metadata": {},
   "source": [
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "l1_ratio = [np.round(0.1*y,1) for y in range(0,11)]\n",
    "param_grid = dict(alpha = alpha, l1_ratio = l1_ratio)\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "991c86d4-23a5-4132-97f8-8672568696c0",
   "metadata": {},
   "source": [
    "estimator = ElasticNet(alpha = results_grid_ind[(target_tic,model,datain)][\"alpha\"], l1_ratio = results_grid_ind[(target_tic,model,datain)][\"l1_ratio\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32895c0a-de30-47f3-986d-fa27e7062252",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "023c9699-759d-4653-9807-41a60eb05745",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893da84-e3a4-4a68-bde8-f1dc4652d9c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.5. Support Vector Machines Regression (SVR)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf89556f-a8c5-48ad-9d0d-03b6521b4e2b",
   "metadata": {},
   "source": [
    "model = \"svm_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = SVR()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fed60742-66ad-421c-b936-e4021b272755",
   "metadata": {},
   "source": [
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "degree = [2,3,4,5]\n",
    "param_grid = dict(kernel = kernel, C = C, degree = degree)\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df9549d7-349b-4883-915f-fc7638782394",
   "metadata": {
    "tags": []
   },
   "source": [
    "# grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cbda02f-36de-47df-9c18-178497de79ec",
   "metadata": {},
   "source": [
    "estimator = SVR(kernel = results_grid_ind[(target_tic,model,datain)][\"kernel\"], \n",
    "              degree = results_grid_ind[(target_tic,model,datain)][\"degree\"],\n",
    "             C = results_grid_ind[(target_tic,model,datain)][\"C\"])\n",
    "\n",
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bda43e53-f2b1-4324-ba1a-3bc9afc8a2c6",
   "metadata": {},
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b989823-10ca-4f82-91ab-529d360953d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.6. SGD Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cc3107d-7a3e-496b-8c1e-870286ac634b",
   "metadata": {},
   "source": [
    "model = \"sdg_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = SGDRegressor()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee923fed-14e8-478a-a7d9-93b0b20f8771",
   "metadata": {},
   "source": [
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "l1_ratio = [np.round(0.1*y,1) for y in range(0,11)]\n",
    "learning_rate = [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"]\n",
    "eta0 = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha = alpha, l1_ratio = l1_ratio, learning_rate = learning_rate, eta0 = eta0)\n",
    "\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a223cd7b-7639-46b5-b1b4-56dc346870bc",
   "metadata": {},
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d5749ae-6e20-496c-bb7e-3d9d5d0dc4bd",
   "metadata": {},
   "source": [
    "estimator = SGDRegressor(alpha = results_grid_ind[(target_tic,model,datain)][\"alpha\"], l1_ratio = results_grid_ind[(target_tic,model,datain)][\"l1_ratio\"],\n",
    "                      learning_rate = results_grid_ind[(target_tic,model,datain)][\"learning_rate\"], eta0 = results_grid_ind[(target_tic,model,datain)][\"eta0\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04bffda4-2c49-4afe-807e-adbd64366d84",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c357fec-6fc2-4cd2-833e-cc36334e2503",
   "metadata": {},
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4d834-b251-42ac-b6cd-797801b576c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.7. BayesianRidge Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b40cb271-8c26-4e0f-b335-cbc10fbc2527",
   "metadata": {},
   "source": [
    "model = \"bayridge_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = BayesianRidge()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8e2824c-8dc7-4f1a-832b-e907c96ea6e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08d8a688-6ed1-4588-9ad0-0c705d3b8f8e",
   "metadata": {},
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2c399-ab14-4f49-b659-490cc311386f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.8. DecisionTree Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a5c5a88-eaf5-4b00-bf0e-673e4d8479ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "model = \"tree_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fa52cd0-2afc-454c-9c41-4b3776c7a4cf",
   "metadata": {},
   "source": [
    "max_depth = [y for y in range(1,16)]\n",
    "param_grid = dict(max_depth = max_depth)\n",
    "\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1, return_train_score = True)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8fe08bc-6032-4f24-91e6-604b27dfd7ea",
   "metadata": {},
   "source": [
    "estimator = DecisionTreeRegressor(max_depth = results_grid_ind[(target_tic,model,datain)][\"max_depth\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "671aa74f-21cc-4b6c-b06d-9a7214907e1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bd14783-da89-4935-931f-aee87c28fa87",
   "metadata": {},
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c036891-7808-4e7c-adba-f4033cb09f28",
   "metadata": {},
   "source": [
    "plt.figure(figsize = (30,15))\n",
    "plot_tree(estimator.fit(Xr_sc,yr), filled = True, rounded = True, feature_names = Xr.columns,fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178c188-3e5c-4c19-9b92-0d0818c339e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.9. RandomForest Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a18f216-fed6-4df4-a973-1a835b4a6678",
   "metadata": {},
   "source": [
    "model = \"ranforest_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f03c7a76-2296-44ab-ad7f-666bdf41eaa6",
   "metadata": {},
   "source": [
    "max_depth = [y for y in range(1,16)]\n",
    "param_grid = dict(max_depth = max_depth)\n",
    "\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "017f435e-85ae-40b6-86d0-cffdf0967f74",
   "metadata": {},
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd6b4062-5456-46ba-9eee-374b6a40845d",
   "metadata": {},
   "source": [
    "estimator = RandomForestRegressor(max_depth = results_grid_ind[(target_tic,model,datain)][\"max_depth\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "879f41d8-6e5d-494e-9e67-dd722b8d91d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecfeec38-6e9c-403d-887d-f6e74185c483",
   "metadata": {},
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadadf2-6ba3-43e1-aa33-3804561b1280",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.10. PolynomialFeatures Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e86c0570-e30d-4931-934d-680db6fe403e",
   "metadata": {},
   "source": [
    "model = \"poly_reg\"\n",
    "datain = \"rets\"\n",
    "\n",
    "estimator = make_pipeline(PolynomialFeatures(), LinearRegression())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b858102-865c-450b-94ad-a3378f8c80ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "param_grid = {'polynomialfeatures__degree': range(2,4)}\n",
    "              # 'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False]}\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c227699-e967-421c-b6b6-714e74d5af71",
   "metadata": {},
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76934f50-ef94-41c8-a737-d3ade2f0bfec",
   "metadata": {},
   "source": [
    "estimator = make_pipeline(PolynomialFeatures(degree = results_grid_ind[(target_tic,model,datain)][\"polynomialfeatures__degree\"]), LinearRegression())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcb0d646-99f0-4cf9-8fd7-1208138552f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2836a585-a9da-4671-ad79-a3cd439c2fca",
   "metadata": {},
   "source": [
    "plot_learning_curves(estimator, Xr_sc, yr, test_size = 0.3, ylim1 = 0, ylim2 = 0.008, title = f\"Learning Curves MSE - R2 | {model}, Train-Test | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d863125-8131-4253-b52f-29728f4ac801",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.11. KNeighbors Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6aa3b9a-65c2-4985-9a5d-7f1edf9df543",
   "metadata": {},
   "source": [
    "model = \"knei_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ca71e64-8abb-46b5-8745-cad8425e5c0b",
   "metadata": {},
   "source": [
    "n_neighbors = [y for y in range(1,20)]\n",
    "param_grid = dict(n_neighbors = n_neighbors)\n",
    "\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9560d2b-c21a-4e01-9a49-ea0edc0e484b",
   "metadata": {},
   "source": [
    "estimator = KNeighborsRegressor(n_neighbors = results_grid_ind[(target_tic,model,datain)][\"n_neighbors\"], n_jobs = -1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d2ed9ef-0d2a-4293-91ef-7c69f948e698",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0.5, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf2589-ac6a-4369-aee5-5bf0b7e45656",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 4.3.2.12. MLP (Neural Net) Regression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cb6be20-990d-4b4a-80d0-1e770d64896e",
   "metadata": {},
   "source": [
    "model = \"mlp_reg\"\n",
    "datain = \"rets\"\n",
    "estimator = MLPRegressor(solver=\"sgd\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "238c984e-de25-49af-bf22-54f2e1a4c4b7",
   "metadata": {},
   "source": [
    "activation = ['identity', 'logistic', 'tanh', 'relu']\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "learning_rate_init = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "param_grid = dict(activation = activation, alpha = alpha, learning_rate_init = learning_rate_init)\n",
    "\n",
    "grid = GridSearchCV(estimator = estimator, param_grid = param_grid, scoring = \"r2\", verbose=1)\n",
    "results_grid_ind[(target_tic,model,datain)] = grid.fit(X,y).best_params_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "274d6b61-fd11-45e1-a4d8-306953a0bebe",
   "metadata": {},
   "source": [
    "estimator = MLPRegressor(solver=\"sgd\",\n",
    "                         activation = results_grid_ind[(target_tic,model,datain)][\"activation\"],\n",
    "                         alpha = results_grid_ind[(target_tic,model,datain)][\"alpha\"],\n",
    "                         learning_rate_init = results_grid_ind[(target_tic,model,datain)][\"learning_rate_init\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfe785f8-3c42-4d6c-b384-32b617fe9a52",
   "metadata": {
    "tags": []
   },
   "source": [
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.3)\n",
    "results_cv_ind[(target_tic,model,datain)] = cross_validate(estimator, Xr_sc, yr, cv = cv, scoring = scoring, return_train_score = True)\n",
    "\n",
    "grouped_barplot_stat(results_cv_ind[(target_tic,model,datain)][\"train_r2\"], results_cv_ind[(target_tic,model,datain)][\"test_r2\"] , ylim1 = 0, title = \"Cross Validation Train-Test r2 score | Returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676e2b2-4e2b-43db-a6a7-aff4018834c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4 Linear Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8877c0e-800f-43b8-8aa7-1c1cbf8e696e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.4.1. Train Test Split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45f9ab8b-9375-4469-8650-13a816636f8e",
   "metadata": {},
   "source": [
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size = 0.3 , random_state = 42)\n",
    "\n",
    "Xr_train_sc = sc.fit_transform(Xr_train)\n",
    "Xr_test_sc = sc.fit_transform(Xr_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6c2e591-6489-4a35-bc41-cee529fcdaa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "size = 10\n",
    "rows = 2\n",
    "cols = int(size/2)\n",
    "samples = np.random.choice(X_train.shape[1], size = size)\n",
    "i = 0\n",
    "fig, axes = plt.subplots(rows,cols,figsize = (20,8))\n",
    "plt.suptitle(\"Train-Test Distros check\", fontsize = 23)\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        ax = axes[row][col]\n",
    "        sns.distplot(X_train.iloc[:,samples[i]], ax=ax, bins = 30, label = \"Train\")\n",
    "        sns.distplot(X_test.iloc[:,samples[i]], ax=ax, bins = 30, label = \"Train\")\n",
    "        ax.legend()\n",
    "        ax.set_xlim(-np.std(X_train.iloc[:,samples[i]])*3, np.std(X_train.iloc[:,samples[i]])*3)\n",
    "        ax.set_title(f\"Sample nÂº: {samples[i]}\")\n",
    "        ax.set_ylabel(None)\n",
    "        i +=1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "313b60f9-4880-44fe-b6e5-56802fe0185c",
   "metadata": {},
   "source": [
    "train_r2_ind = pd.DataFrame(index = results_cv_ind.keys(), columns = [f\"split_{i}\" for i in range(results_cv_ind[list(results_cv_ind)[0]][\"train_r2\"].size)])\n",
    "for k, v in results_cv_ind.items():\n",
    "    train_r2_ind.loc[k] = results_cv_ind[k][\"train_r2\"]\n",
    "train_r2_ind[(\"train_mean\")] = np.round(train_r2_ind.mean(1),2)\n",
    "train_r2_ind[(\"train_std\")] = np.round(train_r2_ind.std(1),3)\n",
    "\n",
    "test_r2_ind = pd.DataFrame(index = results_cv_ind.keys(), columns = [f\"split_{i}\" for i in range(results_cv_ind[list(results_cv_ind)[0]][\"test_r2\"].size)])\n",
    "for k, v in results_cv_ind.items():\n",
    "    test_r2_ind.loc[k] = results_cv_ind[k][\"test_r2\"]\n",
    "test_r2_ind[(\"test_mean\")] = np.round(test_r2_ind.mean(1),2)\n",
    "test_r2_ind[(\"test_std\")] = np.round(test_r2_ind.std(1),3)   \n",
    "\n",
    "train_test_ind = pd.concat([train_r2_ind.iloc[:,-2:],test_r2_ind.iloc[:,-2:]], axis = 1)\n",
    "# train_test[\"diff_mean\"] = train_test[\"train_mean\"]-train_test[\"test_mean\"]\n",
    "# train_test[\"diff_std\"] = train_test[\"train_std\"]-train_test[\"test_std\"]\n",
    "train_test_ind.index.names = [\"ticker\",\"model\",\"datain\"]\n",
    "best_models_ind = train_test_ind.loc[train_test_ind.groupby([\"ticker\"])[\"test_mean\"].idxmax().values,:]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1442ce2c-4731-4711-81d6-4daebefbe16c",
   "metadata": {},
   "source": [
    "print(\"Best model: \")\n",
    "best_models_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-swiss",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sets de entrenamiento y validaciÃ³n \n",
    "# La LSTM se entrenarÃ¡ con datos de 2016 hacia atrÃ¡s. La validaciÃ³n se harÃ¡ con datos de 2017 en adelante.\n",
    "# En ambos casos sÃ³lo se usarÃ¡ el valor mÃ¡s alto de la acciÃ³n para cada dÃ­a\n",
    "\n",
    "# if inter == 3:\n",
    "#     data2 = data.loc[:,(ticks,\"Close\")]\n",
    "# else:\n",
    "#     t = \"d\"\n",
    "#     # start = -365\n",
    "#     data2 = data.resample(f\"1{t}\").mean().loc[:,(ticks,\"Close\")]\n",
    "# tic = ticks[5]\n",
    "# end_train  = \"20-11-2021\"\n",
    "# start_test = \"21-11-2021\"\n",
    "\n",
    "# train_set = data2[tic][\"2019\":end_train]\n",
    "# test_set = data2[tic][start_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-range",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax  = plt.subplots(figsize = (20,6))\n",
    "# ax.plot(train_set,c=\"g\", linewidth = 3)\n",
    "# ax.plot(test_set,c = \"salmon\", linewidth = 3, linestyle = \"dashed\")\n",
    "# ax.legend(['Entrenamiento (2020-2022)', 'ValidaciÃ³n (2022)'], fontsize = 16)\n",
    "# ax.set_xlabel(\"Dates\", fontsize = 14)\n",
    "# ax.set_ylabel(\"Close Price\", fontsize = 14)\n",
    "# ax.set_title(f\"{tic} data to Model\", fontsize = 32, pad= 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-enough",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NormalizaciÃ³n del set de entrenamiento\n",
    "# sc = MinMaxScaler(feature_range=(0,1))\n",
    "# train_set_sc = sc.fit_transform(train_set)\n",
    "# test_set_sc = sc.fit_transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efbfbbd-01c5-4253-9afd-3e5bf9c11422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRID SEARCH TO TUNE IN PARAMETERS\n",
    "\n",
    "# iris = datasets.load_iris()\n",
    "# parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "# svc = svm.SVC()\n",
    "# clf = GridSearchCV(svc, parameters)\n",
    "# clf.fit(iris.data, iris.target)\n",
    "# sorted(clf.cv_results_.keys())\n",
    "\n",
    "# parameters = {'lr':('1e-02', '1e-03','1e-04')}\n",
    "# clf = GridSearchCV(modelo, parameters)\n",
    "# clf.fit(X_train,Y_train,\n",
    "#         epochs=20,\n",
    "#         batch_size=32,\n",
    "#         validation_data = (X_test,Y_test))\n",
    "\n",
    "# sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c2fb7-bed8-4ae0-84fc-9b3e78b20d18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## x. TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c77c41f-adfb-4763-9813-976af2b8f6b1",
   "metadata": {},
   "source": [
    "# PCA\n",
    "pca = PCA(.8)\n",
    "pca.fit(data2y_ret)\n",
    "X_pc = pca.transform(data2y_ret)\n",
    "X_pc_inverse = pca.inverse_transform(X_pc)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cca12dc6-fc32-40a3-923e-9af4647bbb82",
   "metadata": {},
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba04a2b5-72df-42cc-ab37-4166ef649db1",
   "metadata": {},
   "source": [
    "ax=plt.gca()\n",
    "ax.plot(pd.DataFrame(close5y_ret.iloc[:,1]))\n",
    "ax2 = plt.twiny()\n",
    "ax2.plot(pd.DataFrame(X_pc_inverse[:,1]), \"r\", alpha = 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "630af431-93bd-4c6e-8c7a-1ef15a693bd9",
   "metadata": {},
   "source": [
    "data2y_ret.iloc[:,i].name"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32287b05-44a3-45ca-a31e-d9bc850a37b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "pd.DataFrame(X_pc_inverse[:,i],columns = [f\"PC{i}\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71e046d0-f0e5-4591-a0a6-b35e65cef8b0",
   "metadata": {},
   "source": [
    "i = 0\n",
    "pd.concat([data2y_ret.iloc[:,i].reset_index(drop = True),pd.DataFrame(X_pc_inverse[:,i],columns = [f\"PC{i+1}\"]).reset_index(drop = True)],1).corr().iloc[0,1].round(2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2386f5d-84ba-4fdd-9faa-807151150153",
   "metadata": {},
   "source": [
    "i = 1\n",
    "def test (i):\n",
    "    plt.scatter(pd.DataFrame(close5y_ret.iloc[:,i]),pd.DataFrame(X_pc_inverse[:,i]))\n",
    "    return f\"{close5y_ret.iloc[:,i].name}: {pd.concat([close5y_ret.iloc[:,i].reset_index(drop = True),pd.DataFrame(X_pc_inverse[:,i],columns = [f'PC{i+1}']).reset_index(drop = True)],1).corr().iloc[0,1].round(2)}\"\n",
    "    plt.show()\n",
    "    \n",
    "interact(test,i = widgets.IntSlider(min = 0 , max = close5y_ret.shape[1]-1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9dbee1f3-2082-4ed6-b5bb-e4a245408c87",
   "metadata": {},
   "source": [
    "X_pc.shape, X_pc_inverse.shape, close5y_ret.T.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48026819-1b8e-4d2c-9efe-932ecab7f0f6",
   "metadata": {},
   "source": [
    "X_pc_df = pd.DataFrame(X_pc, columns = [f\"PC{i}\" for i in range(1,X_pc.shape[1]+1)], index = close5y_ret.T.index)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f278bbfd-1003-4773-8708-2b8b561a6934",
   "metadata": {},
   "source": [
    "pc1 = pd.Series(data = pca.components_[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9834679f-e332-41b2-b557-bfde02fef3ce",
   "metadata": {},
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d24d76c3-911b-429e-9a86-888c12dabcf8",
   "metadata": {},
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "156ee07e-755d-4cfa-b372-6dfb92faa4ce",
   "metadata": {},
   "source": [
    "plt.plot(pca.explained_variance_)\n",
    "pc1.nsmallest(10),pc1.nlargest(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afadd43d-8c03-43be-b3be-253552da7369",
   "metadata": {},
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "690e7159-99b4-4927-b10e-ff36308c7eb7",
   "metadata": {},
   "source": [
    "plt.plot(pca.components_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3593f6dc-500a-4bdd-8cbb-fa777eaff977",
   "metadata": {},
   "source": [
    "data2y_ret.iloc[:,0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6407d9dd-4753-4a09-a337-39c20558e826",
   "metadata": {},
   "source": [
    "# weights = abs(pc1)/sum(abs(pc1))\n",
    "# myrs = (weights*close5y_ret).sum(1)\n",
    "\n",
    "rs_df = pd.concat([pd.Series(X_pc[:,0]),data2y_ret.T.iloc[:,0]],1)\n",
    "rs_df.columns = [\"PCA\",\"AAPL\"]\n",
    "\n",
    "crs_df = rs_df.cumsum().apply(np.exp)\n",
    "crs_df.plot(subplots=True,sharex=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5631e98-5bca-4de5-8045-ca4c350b5778",
   "metadata": {},
   "source": [
    "# Initialize linear regression instance\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Create empty list to store RMSE for each iteration\n",
    "rmse_list = []\n",
    "\n",
    "# Loop through different count of principal components for linear regression\n",
    "for i in range(1, X_train_pc.shape[1]+1):\n",
    "    rmse_score = -1 * cross_val_score(lin_reg, \n",
    "                                      X_train_pc[:,:i], # Use first k principal components\n",
    "                                      y_train, \n",
    "                                      cv=cv, \n",
    "                                      scoring='neg_root_mean_squared_error').mean()\n",
    "    rmse_list.append(rmse_score)\n",
    "    \n",
    "# Visual analysis - plot RMSE vs count of principal components used\n",
    "plt.plot(rmse_list, '-o')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Quality')\n",
    "plt.xlim(xmin=-1);\n",
    "plt.xticks(np.arange(X_train_pc.shape[1]), np.arange(1, X_train_pc.shape[1]+1))\n",
    "plt.axhline(y=lr_score_train, color='g', linestyle='-');"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8c07219-891b-44f6-9126-7c2fccd185cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "43f834fd-6b56-49fd-a317-64fa77d10616",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4814f138-45ed-42fa-9ecc-246ed78cfe71",
   "metadata": {},
   "source": [
    "pca_pipe = make_pipeline(StandardScaler(), PCA())\n",
    "pca_pipe.fit(close5y_ret)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "885484e5-11c1-4a2d-9908-155f658666b1",
   "metadata": {},
   "source": [
    "pca_model = pca_pipe.named_steps['pca']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cb22d83-f840-460c-b315-a0c9b1e8db32",
   "metadata": {},
   "source": [
    "pd.DataFrame(data = pca_model.components_,\n",
    "            columns = close_ret.columns,)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "133afab4-1b07-4774-bf6f-c158164f460d",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize = (20,15))\n",
    "sns.heatmap(data = pca_model.components_,cmap = \"viridis\", yticklabels = close_ret.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3dcb05c-87a0-4208-8d19-40031b96d2c0",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(25, 8))\n",
    "ax.bar(\n",
    "    x      = np.arange(pca_model.n_components_) + 1,\n",
    "    height = pca_model.explained_variance_ratio_\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(len(close_ret.columns)) + 1, pca_model.explained_variance_ratio_):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x,y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "ax.set_xticks(np.arange(pca_model.n_components_) + 1)\n",
    "# ax.set_ylim(0, 1.1)\n",
    "ax.set_title('Porcentaje de varianza explicada por cada componente', fontsize = 24)\n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Por. varianza explicada');"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce6492e7-0842-4248-b09e-e34bd62953cb",
   "metadata": {},
   "source": [
    "prop_varianza_acum = pca_model.explained_variance_ratio_.cumsum()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(25, 8))\n",
    "ax.plot(\n",
    "    np.arange(len(close_ret.columns)) + 1,\n",
    "    prop_varianza_acum,\n",
    "    marker = 'o'\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(len(close_ret.columns)) + 1, prop_varianza_acum):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x,y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha='center'\n",
    "    )\n",
    "    \n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_xticks(np.arange(pca_model.n_components_) + 1)\n",
    "ax.set_title('Porcentaje de varianza explicada acumulada', fontsize = 24)\n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Por. varianza acumulada');"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80ca1136-1aca-49aa-a1a1-d043f4b5b4b1",
   "metadata": {},
   "source": [
    "proyeccion.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d568892-93bf-4e54-ac14-129c1c74e46e",
   "metadata": {},
   "source": [
    "proyeccion = pca_pipe.transform(X = close_ret)\n",
    "pd.DataFrame(proyeccion,\n",
    "             columns = np.arange(close_ret.columns.size),\n",
    "            index = close_ret.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63d3e75f-b74e-4b78-9a61-b5685ab39b20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88177718-eb54-4fa7-86bf-38d388f1d82c",
   "metadata": {
    "tags": []
   },
   "source": [
    "%who_ls DataFrame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
